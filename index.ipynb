{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I want to file a \" Bait and Switch '' Complain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I am an account holder for my personal, busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>they took my whole social security check i had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>This is in dispute of my Case number : XXXX. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>My Bluebird card that i used for bill pay was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                       Consumer complaint narrative\n",
       "0            Consumer Loan  I want to file a \" Bait and Switch '' Complain...\n",
       "1  Bank account or service  I am an account holder for my personal, busine...\n",
       "2  Bank account or service  they took my whole social security check i had...\n",
       "3  Bank account or service  This is in dispute of my Case number : XXXX. I...\n",
       "4  Bank account or service  My Bluebird card that i used for bill pay was ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "df_ds = df.sample(10000).reset_index(drop=True)\n",
    "df_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = df_ds['Product']\n",
    "complaints = df_ds['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(2000)\n",
    "tokenizer.fit_on_texts(complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(complaints)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(products)\n",
    "prod_cats = le.transform(products)\n",
    "\n",
    "products_onehot = to_categorical(prod_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, products_onehot, test_size = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "np.random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation = 'relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation = 'relu'))\n",
    "model.add(layers.Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.9596 - acc: 0.1589 - val_loss: 1.9391 - val_acc: 0.1600\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.9311 - acc: 0.1907 - val_loss: 1.9207 - val_acc: 0.1880\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.9140 - acc: 0.2129 - val_loss: 1.9063 - val_acc: 0.1980\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.8978 - acc: 0.2284 - val_loss: 1.8916 - val_acc: 0.2180\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.8800 - acc: 0.2460 - val_loss: 1.8754 - val_acc: 0.2330\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.8605 - acc: 0.2569 - val_loss: 1.8571 - val_acc: 0.2480\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8392 - acc: 0.2672 - val_loss: 1.8369 - val_acc: 0.2620\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8160 - acc: 0.2820 - val_loss: 1.8139 - val_acc: 0.2940\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.7909 - acc: 0.3033 - val_loss: 1.7887 - val_acc: 0.3150\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.7634 - acc: 0.3296 - val_loss: 1.7614 - val_acc: 0.3290\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7335 - acc: 0.3448 - val_loss: 1.7310 - val_acc: 0.3540\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7005 - acc: 0.3696 - val_loss: 1.6979 - val_acc: 0.3750\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6642 - acc: 0.3927 - val_loss: 1.6612 - val_acc: 0.4110\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6250 - acc: 0.4244 - val_loss: 1.6230 - val_acc: 0.4210\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5832 - acc: 0.4508 - val_loss: 1.5809 - val_acc: 0.4520\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5384 - acc: 0.4844 - val_loss: 1.5363 - val_acc: 0.4690\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4910 - acc: 0.5089 - val_loss: 1.4881 - val_acc: 0.5260\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4415 - acc: 0.5421 - val_loss: 1.4389 - val_acc: 0.5600\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3902 - acc: 0.5685 - val_loss: 1.3903 - val_acc: 0.5780\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.3382 - acc: 0.5907 - val_loss: 1.3394 - val_acc: 0.6080\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2870 - acc: 0.6107 - val_loss: 1.2922 - val_acc: 0.6190\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2377 - acc: 0.6289 - val_loss: 1.2440 - val_acc: 0.6340\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1905 - acc: 0.6443 - val_loss: 1.2000 - val_acc: 0.6470\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1454 - acc: 0.6625 - val_loss: 1.1585 - val_acc: 0.6450\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1034 - acc: 0.6719 - val_loss: 1.1199 - val_acc: 0.6600\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0643 - acc: 0.6843 - val_loss: 1.0846 - val_acc: 0.6590\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0275 - acc: 0.6909 - val_loss: 1.0491 - val_acc: 0.6670\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9938 - acc: 0.6984 - val_loss: 1.0181 - val_acc: 0.6740\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9622 - acc: 0.7055 - val_loss: 0.9919 - val_acc: 0.6800\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9337 - acc: 0.7121 - val_loss: 0.9657 - val_acc: 0.6870\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9078 - acc: 0.7180 - val_loss: 0.9423 - val_acc: 0.6820\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8836 - acc: 0.7211 - val_loss: 0.9241 - val_acc: 0.6770\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8612 - acc: 0.7248 - val_loss: 0.9043 - val_acc: 0.6820\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8407 - acc: 0.7315 - val_loss: 0.8872 - val_acc: 0.7000\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8220 - acc: 0.7339 - val_loss: 0.8695 - val_acc: 0.7010\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8043 - acc: 0.7371 - val_loss: 0.8549 - val_acc: 0.7040\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7881 - acc: 0.7419 - val_loss: 0.8430 - val_acc: 0.7040\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7730 - acc: 0.7425 - val_loss: 0.8293 - val_acc: 0.7060\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7589 - acc: 0.7461 - val_loss: 0.8196 - val_acc: 0.7120\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7463 - acc: 0.7512 - val_loss: 0.8106 - val_acc: 0.7080\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7339 - acc: 0.7512 - val_loss: 0.8012 - val_acc: 0.7090\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7223 - acc: 0.7553 - val_loss: 0.7922 - val_acc: 0.7090\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7116 - acc: 0.7595 - val_loss: 0.7839 - val_acc: 0.7130\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7011 - acc: 0.7643 - val_loss: 0.7757 - val_acc: 0.7130\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6913 - acc: 0.7657 - val_loss: 0.7690 - val_acc: 0.7120\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6823 - acc: 0.7673 - val_loss: 0.7635 - val_acc: 0.7190\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6732 - acc: 0.7733 - val_loss: 0.7551 - val_acc: 0.7240\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6651 - acc: 0.7735 - val_loss: 0.7507 - val_acc: 0.7210\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6568 - acc: 0.7748 - val_loss: 0.7473 - val_acc: 0.7260\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6493 - acc: 0.7793 - val_loss: 0.7411 - val_acc: 0.7270\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6413 - acc: 0.7832 - val_loss: 0.7383 - val_acc: 0.7310\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6352 - acc: 0.7812 - val_loss: 0.7297 - val_acc: 0.7330\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6273 - acc: 0.7851 - val_loss: 0.7274 - val_acc: 0.7280\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6208 - acc: 0.7867 - val_loss: 0.7243 - val_acc: 0.7310\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6144 - acc: 0.7907 - val_loss: 0.7184 - val_acc: 0.7340\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6078 - acc: 0.7907 - val_loss: 0.7152 - val_acc: 0.7360\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6023 - acc: 0.7943 - val_loss: 0.7113 - val_acc: 0.7340\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5963 - acc: 0.7943 - val_loss: 0.7089 - val_acc: 0.7370\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5901 - acc: 0.7963 - val_loss: 0.7102 - val_acc: 0.7340\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5852 - acc: 0.7993 - val_loss: 0.7009 - val_acc: 0.7330\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5797 - acc: 0.8009 - val_loss: 0.6997 - val_acc: 0.7370\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5737 - acc: 0.8040 - val_loss: 0.6973 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5691 - acc: 0.8060 - val_loss: 0.6956 - val_acc: 0.7430\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5630 - acc: 0.8075 - val_loss: 0.6915 - val_acc: 0.7380\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5578 - acc: 0.8105 - val_loss: 0.6869 - val_acc: 0.7420\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5533 - acc: 0.8120 - val_loss: 0.6858 - val_acc: 0.7430\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5481 - acc: 0.8147 - val_loss: 0.6836 - val_acc: 0.7450\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5436 - acc: 0.8161 - val_loss: 0.6820 - val_acc: 0.7410\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5390 - acc: 0.8177 - val_loss: 0.6816 - val_acc: 0.7390\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5343 - acc: 0.8188 - val_loss: 0.6775 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5295 - acc: 0.8228 - val_loss: 0.6751 - val_acc: 0.7400\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5252 - acc: 0.8228 - val_loss: 0.6746 - val_acc: 0.7500\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5209 - acc: 0.8256 - val_loss: 0.6721 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5164 - acc: 0.8251 - val_loss: 0.6707 - val_acc: 0.7520\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5121 - acc: 0.8276 - val_loss: 0.6676 - val_acc: 0.7490\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5083 - acc: 0.8315 - val_loss: 0.6674 - val_acc: 0.7500\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5040 - acc: 0.8305 - val_loss: 0.6694 - val_acc: 0.7450\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5001 - acc: 0.8328 - val_loss: 0.6657 - val_acc: 0.7480\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4960 - acc: 0.8347 - val_loss: 0.6639 - val_acc: 0.7540\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4922 - acc: 0.8341 - val_loss: 0.6644 - val_acc: 0.7470\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4880 - acc: 0.8372 - val_loss: 0.6637 - val_acc: 0.7430\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4847 - acc: 0.8375 - val_loss: 0.6649 - val_acc: 0.7470\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4807 - acc: 0.8392 - val_loss: 0.6593 - val_acc: 0.7580\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4768 - acc: 0.8413 - val_loss: 0.6598 - val_acc: 0.7530\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4721 - acc: 0.8435 - val_loss: 0.6592 - val_acc: 0.7560\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4688 - acc: 0.8464 - val_loss: 0.6597 - val_acc: 0.7520\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4654 - acc: 0.8471 - val_loss: 0.6574 - val_acc: 0.7550\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4620 - acc: 0.8469 - val_loss: 0.6558 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4581 - acc: 0.8509 - val_loss: 0.6525 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4547 - acc: 0.8495 - val_loss: 0.6540 - val_acc: 0.7530\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4513 - acc: 0.8516 - val_loss: 0.6584 - val_acc: 0.7490\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4476 - acc: 0.8528 - val_loss: 0.6524 - val_acc: 0.7600\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4440 - acc: 0.8519 - val_loss: 0.6522 - val_acc: 0.7450\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4410 - acc: 0.8557 - val_loss: 0.6509 - val_acc: 0.7540\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4378 - acc: 0.8557 - val_loss: 0.6516 - val_acc: 0.7430\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4340 - acc: 0.8575 - val_loss: 0.6504 - val_acc: 0.7510\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4303 - acc: 0.8593 - val_loss: 0.6480 - val_acc: 0.7500\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4274 - acc: 0.8592 - val_loss: 0.6482 - val_acc: 0.7480\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4242 - acc: 0.8625 - val_loss: 0.6505 - val_acc: 0.7460\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4209 - acc: 0.8616 - val_loss: 0.6494 - val_acc: 0.7490\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4178 - acc: 0.8647 - val_loss: 0.6484 - val_acc: 0.7540\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4150 - acc: 0.8660 - val_loss: 0.6470 - val_acc: 0.7480\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4115 - acc: 0.8660 - val_loss: 0.6481 - val_acc: 0.7470\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4087 - acc: 0.8687 - val_loss: 0.6460 - val_acc: 0.7490\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4053 - acc: 0.8677 - val_loss: 0.6487 - val_acc: 0.7510\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4024 - acc: 0.8712 - val_loss: 0.6496 - val_acc: 0.7520\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3992 - acc: 0.8708 - val_loss: 0.6498 - val_acc: 0.7500\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3967 - acc: 0.8727 - val_loss: 0.6449 - val_acc: 0.7470\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3929 - acc: 0.8755 - val_loss: 0.6462 - val_acc: 0.7450\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3903 - acc: 0.8772 - val_loss: 0.6455 - val_acc: 0.7500\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.3873 - acc: 0.8775 - val_loss: 0.6512 - val_acc: 0.7510\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.3840 - acc: 0.8784 - val_loss: 0.6470 - val_acc: 0.7410\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.3818 - acc: 0.8776 - val_loss: 0.6463 - val_acc: 0.7460\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3787 - acc: 0.8791 - val_loss: 0.6457 - val_acc: 0.7490\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3757 - acc: 0.8811 - val_loss: 0.6458 - val_acc: 0.7540\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.3731 - acc: 0.8836 - val_loss: 0.6518 - val_acc: 0.7470\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3709 - acc: 0.8827 - val_loss: 0.6454 - val_acc: 0.7510\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.3675 - acc: 0.8840 - val_loss: 0.6465 - val_acc: 0.7480\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3646 - acc: 0.8869 - val_loss: 0.6520 - val_acc: 0.7480\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3622 - acc: 0.8876 - val_loss: 0.6490 - val_acc: 0.7520\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                      label_train_final,\n",
    "                      epochs=120,\n",
    "                      batch_size=256,\n",
    "                      validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35941041836738585, 0.8881333333333333]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6453050337632498, 0.7586666668256123]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9596 - acc: 0.1589 - val_loss: 1.9391 - val_acc: 0.1600\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.9311 - acc: 0.1909 - val_loss: 1.9207 - val_acc: 0.1890\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9140 - acc: 0.2132 - val_loss: 1.9063 - val_acc: 0.1980\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8977 - acc: 0.2284 - val_loss: 1.8916 - val_acc: 0.2190\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8799 - acc: 0.2457 - val_loss: 1.8754 - val_acc: 0.2330\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8605 - acc: 0.2569 - val_loss: 1.8571 - val_acc: 0.2470\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8392 - acc: 0.2672 - val_loss: 1.8369 - val_acc: 0.2610\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8160 - acc: 0.2817 - val_loss: 1.8139 - val_acc: 0.2940\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7909 - acc: 0.3028 - val_loss: 1.7887 - val_acc: 0.3150\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7635 - acc: 0.3300 - val_loss: 1.7615 - val_acc: 0.3290\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7335 - acc: 0.3451 - val_loss: 1.7310 - val_acc: 0.3530\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7006 - acc: 0.3696 - val_loss: 1.6980 - val_acc: 0.3740\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6642 - acc: 0.3929 - val_loss: 1.6612 - val_acc: 0.4120\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6250 - acc: 0.4241 - val_loss: 1.6230 - val_acc: 0.4210\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5832 - acc: 0.4509 - val_loss: 1.5808 - val_acc: 0.4530\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5384 - acc: 0.4847 - val_loss: 1.5363 - val_acc: 0.4700\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4909 - acc: 0.5092 - val_loss: 1.4880 - val_acc: 0.5270\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4414 - acc: 0.5424 - val_loss: 1.4389 - val_acc: 0.5580\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3902 - acc: 0.5691 - val_loss: 1.3903 - val_acc: 0.5790\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3383 - acc: 0.5908 - val_loss: 1.3395 - val_acc: 0.6090\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2872 - acc: 0.6113 - val_loss: 1.2923 - val_acc: 0.6200\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2378 - acc: 0.6292 - val_loss: 1.2441 - val_acc: 0.6340\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1906 - acc: 0.6445 - val_loss: 1.2001 - val_acc: 0.6470\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1455 - acc: 0.6624 - val_loss: 1.1586 - val_acc: 0.6450\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1035 - acc: 0.6719 - val_loss: 1.1200 - val_acc: 0.6600\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0644 - acc: 0.6841 - val_loss: 1.0847 - val_acc: 0.6580\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0275 - acc: 0.6905 - val_loss: 1.0492 - val_acc: 0.6680\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9938 - acc: 0.6989 - val_loss: 1.0181 - val_acc: 0.6740\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9623 - acc: 0.7049 - val_loss: 0.9920 - val_acc: 0.6790\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9338 - acc: 0.7124 - val_loss: 0.9658 - val_acc: 0.6870\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9079 - acc: 0.7179 - val_loss: 0.9424 - val_acc: 0.6830\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8836 - acc: 0.7213 - val_loss: 0.9242 - val_acc: 0.6770\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8612 - acc: 0.7251 - val_loss: 0.9043 - val_acc: 0.6820\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.7316 - val_loss: 0.8873 - val_acc: 0.7010\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8220 - acc: 0.7336 - val_loss: 0.8696 - val_acc: 0.7010\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8044 - acc: 0.7369 - val_loss: 0.8550 - val_acc: 0.7040\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7881 - acc: 0.7417 - val_loss: 0.8430 - val_acc: 0.7040\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7731 - acc: 0.7431 - val_loss: 0.8293 - val_acc: 0.7050\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7589 - acc: 0.7463 - val_loss: 0.8197 - val_acc: 0.7120\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7463 - acc: 0.7508 - val_loss: 0.8106 - val_acc: 0.7080\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7339 - acc: 0.7512 - val_loss: 0.8013 - val_acc: 0.7090\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7224 - acc: 0.7555 - val_loss: 0.7923 - val_acc: 0.7090\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7116 - acc: 0.7593 - val_loss: 0.7839 - val_acc: 0.7130\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7012 - acc: 0.7644 - val_loss: 0.7757 - val_acc: 0.7130\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6914 - acc: 0.7657 - val_loss: 0.7690 - val_acc: 0.7120\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6823 - acc: 0.7671 - val_loss: 0.7635 - val_acc: 0.7200\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6732 - acc: 0.7736 - val_loss: 0.7551 - val_acc: 0.7240\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6652 - acc: 0.7735 - val_loss: 0.7507 - val_acc: 0.7210\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6568 - acc: 0.7744 - val_loss: 0.7474 - val_acc: 0.7260\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6493 - acc: 0.7792 - val_loss: 0.7411 - val_acc: 0.7260\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6414 - acc: 0.7831 - val_loss: 0.7384 - val_acc: 0.7320\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6352 - acc: 0.7812 - val_loss: 0.7298 - val_acc: 0.7330\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6273 - acc: 0.7849 - val_loss: 0.7274 - val_acc: 0.7280\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6208 - acc: 0.7867 - val_loss: 0.7243 - val_acc: 0.7310\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6145 - acc: 0.7905 - val_loss: 0.7184 - val_acc: 0.7350\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6079 - acc: 0.7908 - val_loss: 0.7152 - val_acc: 0.7360\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6024 - acc: 0.7944 - val_loss: 0.7113 - val_acc: 0.7340\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5964 - acc: 0.7943 - val_loss: 0.7089 - val_acc: 0.7370\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5901 - acc: 0.7964 - val_loss: 0.7103 - val_acc: 0.7330\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5852 - acc: 0.7993 - val_loss: 0.7009 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5821890726089477, 0.8]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6963947723706563, 0.7393333330154419]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.6142 - acc: 0.1589 - val_loss: 2.5916 - val_acc: 0.1600\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5818 - acc: 0.1909 - val_loss: 2.5695 - val_acc: 0.1880\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5611 - acc: 0.2131 - val_loss: 2.5516 - val_acc: 0.2000\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.5414 - acc: 0.2285 - val_loss: 2.5335 - val_acc: 0.2200\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5204 - acc: 0.2451 - val_loss: 2.5141 - val_acc: 0.2320\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.4979 - acc: 0.2567 - val_loss: 2.4929 - val_acc: 0.2460\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4738 - acc: 0.2665 - val_loss: 2.4699 - val_acc: 0.2620\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4482 - acc: 0.2815 - val_loss: 2.4448 - val_acc: 0.2890\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4211 - acc: 0.2997 - val_loss: 2.4177 - val_acc: 0.3150\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3919 - acc: 0.3265 - val_loss: 2.3888 - val_acc: 0.3280\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3607 - acc: 0.3396 - val_loss: 2.3572 - val_acc: 0.3460\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.3267 - acc: 0.3637 - val_loss: 2.3233 - val_acc: 0.3710\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2898 - acc: 0.3864 - val_loss: 2.2862 - val_acc: 0.4030\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.2505 - acc: 0.4193 - val_loss: 2.2479 - val_acc: 0.4130\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2090 - acc: 0.4441 - val_loss: 2.2063 - val_acc: 0.4490\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1649 - acc: 0.4768 - val_loss: 2.1626 - val_acc: 0.4640\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1184 - acc: 0.4993 - val_loss: 2.1150 - val_acc: 0.5180\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0700 - acc: 0.5345 - val_loss: 2.0671 - val_acc: 0.5480\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0199 - acc: 0.5616 - val_loss: 2.0195 - val_acc: 0.5660\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9691 - acc: 0.5840 - val_loss: 1.9698 - val_acc: 0.6050\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9189 - acc: 0.6057 - val_loss: 1.9230 - val_acc: 0.6190\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8703 - acc: 0.6211 - val_loss: 1.8753 - val_acc: 0.6270\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8233 - acc: 0.6381 - val_loss: 1.8309 - val_acc: 0.6420\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7781 - acc: 0.6561 - val_loss: 1.7888 - val_acc: 0.6520\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7356 - acc: 0.6667 - val_loss: 1.7493 - val_acc: 0.6550\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6957 - acc: 0.6772 - val_loss: 1.7124 - val_acc: 0.6590\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6578 - acc: 0.6863 - val_loss: 1.6757 - val_acc: 0.6660\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6227 - acc: 0.6937 - val_loss: 1.6428 - val_acc: 0.6730\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5896 - acc: 0.7004 - val_loss: 1.6145 - val_acc: 0.6780\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5594 - acc: 0.7059 - val_loss: 1.5862 - val_acc: 0.6870\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5317 - acc: 0.7147 - val_loss: 1.5607 - val_acc: 0.6830\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5057 - acc: 0.7197 - val_loss: 1.5401 - val_acc: 0.6810\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4814 - acc: 0.7219 - val_loss: 1.5184 - val_acc: 0.6860\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4590 - acc: 0.7276 - val_loss: 1.4987 - val_acc: 0.7020\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4384 - acc: 0.7292 - val_loss: 1.4789 - val_acc: 0.6990\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4189 - acc: 0.7348 - val_loss: 1.4619 - val_acc: 0.7080\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4008 - acc: 0.7389 - val_loss: 1.4480 - val_acc: 0.7030\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3839 - acc: 0.7396 - val_loss: 1.4320 - val_acc: 0.7060\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3679 - acc: 0.7433 - val_loss: 1.4201 - val_acc: 0.7180\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3535 - acc: 0.7480 - val_loss: 1.4084 - val_acc: 0.7080\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3392 - acc: 0.7489 - val_loss: 1.3971 - val_acc: 0.7070\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3259 - acc: 0.7507 - val_loss: 1.3858 - val_acc: 0.7090\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3133 - acc: 0.7556 - val_loss: 1.3753 - val_acc: 0.7080\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3012 - acc: 0.7603 - val_loss: 1.3647 - val_acc: 0.7140\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2896 - acc: 0.7615 - val_loss: 1.3562 - val_acc: 0.7100\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2788 - acc: 0.7648 - val_loss: 1.3484 - val_acc: 0.7160\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2680 - acc: 0.7692 - val_loss: 1.3379 - val_acc: 0.7260\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2582 - acc: 0.7693 - val_loss: 1.3311 - val_acc: 0.7200\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2481 - acc: 0.7711 - val_loss: 1.3253 - val_acc: 0.7300\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2390 - acc: 0.7749 - val_loss: 1.3171 - val_acc: 0.7300\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2294 - acc: 0.7777 - val_loss: 1.3117 - val_acc: 0.7320\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2214 - acc: 0.7787 - val_loss: 1.3016 - val_acc: 0.7320\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2120 - acc: 0.7804 - val_loss: 1.2965 - val_acc: 0.7290\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2039 - acc: 0.7825 - val_loss: 1.2912 - val_acc: 0.7290\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1959 - acc: 0.7835 - val_loss: 1.2836 - val_acc: 0.7340\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1877 - acc: 0.7869 - val_loss: 1.2782 - val_acc: 0.7370\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1805 - acc: 0.7884 - val_loss: 1.2723 - val_acc: 0.7280\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1730 - acc: 0.7891 - val_loss: 1.2676 - val_acc: 0.7360\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1652 - acc: 0.7915 - val_loss: 1.2661 - val_acc: 0.7360\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1587 - acc: 0.7947 - val_loss: 1.2558 - val_acc: 0.7380\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1517 - acc: 0.7951 - val_loss: 1.2521 - val_acc: 0.7380\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1442 - acc: 0.7963 - val_loss: 1.2479 - val_acc: 0.7380\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1381 - acc: 0.8001 - val_loss: 1.2431 - val_acc: 0.7400\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1306 - acc: 0.8021 - val_loss: 1.2376 - val_acc: 0.7450\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1239 - acc: 0.8029 - val_loss: 1.2315 - val_acc: 0.7490\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1179 - acc: 0.8051 - val_loss: 1.2279 - val_acc: 0.7470\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1113 - acc: 0.8089 - val_loss: 1.2233 - val_acc: 0.7450\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1053 - acc: 0.8096 - val_loss: 1.2197 - val_acc: 0.7420\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0993 - acc: 0.8101 - val_loss: 1.2168 - val_acc: 0.7460\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0931 - acc: 0.8099 - val_loss: 1.2111 - val_acc: 0.7440\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0870 - acc: 0.8135 - val_loss: 1.2068 - val_acc: 0.7460\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0812 - acc: 0.8139 - val_loss: 1.2038 - val_acc: 0.7490\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0755 - acc: 0.8173 - val_loss: 1.1992 - val_acc: 0.7500\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0698 - acc: 0.8161 - val_loss: 1.1957 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0641 - acc: 0.8191 - val_loss: 1.1908 - val_acc: 0.7510\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0589 - acc: 0.8219 - val_loss: 1.1883 - val_acc: 0.7510\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0533 - acc: 0.8215 - val_loss: 1.1876 - val_acc: 0.7460\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0481 - acc: 0.8248 - val_loss: 1.1825 - val_acc: 0.7510\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0427 - acc: 0.8263 - val_loss: 1.1784 - val_acc: 0.7550\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0375 - acc: 0.8272 - val_loss: 1.1765 - val_acc: 0.7440\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0321 - acc: 0.8295 - val_loss: 1.1742 - val_acc: 0.7480\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0276 - acc: 0.8292 - val_loss: 1.1728 - val_acc: 0.7510\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0223 - acc: 0.8300 - val_loss: 1.1653 - val_acc: 0.7530\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0171 - acc: 0.8327 - val_loss: 1.1638 - val_acc: 0.7530\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0114 - acc: 0.8352 - val_loss: 1.1610 - val_acc: 0.7540\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0068 - acc: 0.8347 - val_loss: 1.1596 - val_acc: 0.7550\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0021 - acc: 0.8361 - val_loss: 1.1546 - val_acc: 0.7570\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9975 - acc: 0.8383 - val_loss: 1.1514 - val_acc: 0.7600\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9926 - acc: 0.8393 - val_loss: 1.1473 - val_acc: 0.7530\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9879 - acc: 0.8407 - val_loss: 1.1456 - val_acc: 0.7580\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9833 - acc: 0.8407 - val_loss: 1.1475 - val_acc: 0.7510\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9784 - acc: 0.8433 - val_loss: 1.1397 - val_acc: 0.7650\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9737 - acc: 0.8423 - val_loss: 1.1379 - val_acc: 0.7610\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.8459 - val_loss: 1.1349 - val_acc: 0.7620\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9651 - acc: 0.8461 - val_loss: 1.1338 - val_acc: 0.7530\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9605 - acc: 0.8471 - val_loss: 1.1306 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.8491 - val_loss: 1.1264 - val_acc: 0.7610\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9514 - acc: 0.8488 - val_loss: 1.1242 - val_acc: 0.7580\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9473 - acc: 0.8516 - val_loss: 1.1243 - val_acc: 0.7560\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9430 - acc: 0.8516 - val_loss: 1.1213 - val_acc: 0.7650\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9388 - acc: 0.8529 - val_loss: 1.1178 - val_acc: 0.7630\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9349 - acc: 0.8553 - val_loss: 1.1148 - val_acc: 0.7650\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9304 - acc: 0.8539 - val_loss: 1.1141 - val_acc: 0.7570\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9265 - acc: 0.8567 - val_loss: 1.1100 - val_acc: 0.7550\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9221 - acc: 0.8597 - val_loss: 1.1093 - val_acc: 0.7610\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9182 - acc: 0.8585 - val_loss: 1.1083 - val_acc: 0.7630\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9139 - acc: 0.8597 - val_loss: 1.1080 - val_acc: 0.7580\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9104 - acc: 0.8608 - val_loss: 1.1008 - val_acc: 0.7610\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9058 - acc: 0.8649 - val_loss: 1.1003 - val_acc: 0.7650\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9021 - acc: 0.8635 - val_loss: 1.0975 - val_acc: 0.7610\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8983 - acc: 0.8641 - val_loss: 1.0997 - val_acc: 0.7580\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8939 - acc: 0.8667 - val_loss: 1.0946 - val_acc: 0.7610\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8906 - acc: 0.8641 - val_loss: 1.0924 - val_acc: 0.7600\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8865 - acc: 0.8669 - val_loss: 1.0895 - val_acc: 0.7620\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8826 - acc: 0.8692 - val_loss: 1.0877 - val_acc: 0.7610\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8791 - acc: 0.8697 - val_loss: 1.0902 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8759 - acc: 0.8720 - val_loss: 1.0834 - val_acc: 0.7590\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8717 - acc: 0.8713 - val_loss: 1.0822 - val_acc: 0.7590\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8679 - acc: 0.8740 - val_loss: 1.0847 - val_acc: 0.7600\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8644 - acc: 0.8741 - val_loss: 1.0812 - val_acc: 0.7570\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                     label_train_final,\n",
    "                     epochs=120,\n",
    "                     batch_size=256,\n",
    "                     validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 16.0394 - acc: 0.1595 - val_loss: 15.6246 - val_acc: 0.1620\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 15.2714 - acc: 0.1924 - val_loss: 14.8786 - val_acc: 0.1870\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 14.5372 - acc: 0.2153 - val_loss: 14.1588 - val_acc: 0.2050\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 13.8259 - acc: 0.2327 - val_loss: 13.4607 - val_acc: 0.2170\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 13.1352 - acc: 0.2440 - val_loss: 12.7826 - val_acc: 0.2340\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 12.4647 - acc: 0.2529 - val_loss: 12.1247 - val_acc: 0.2490\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 11.8149 - acc: 0.2607 - val_loss: 11.4869 - val_acc: 0.2650\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1858 - acc: 0.2705 - val_loss: 10.8691 - val_acc: 0.2770\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 10.5774 - acc: 0.2872 - val_loss: 10.2716 - val_acc: 0.3020\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 9.9888 - acc: 0.3077 - val_loss: 9.6943 - val_acc: 0.3070\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 9.4199 - acc: 0.3131 - val_loss: 9.1366 - val_acc: 0.3240\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 8.8714 - acc: 0.3363 - val_loss: 8.6004 - val_acc: 0.3250\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 8.3446 - acc: 0.3567 - val_loss: 8.0853 - val_acc: 0.3840\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 7.8388 - acc: 0.3951 - val_loss: 7.5921 - val_acc: 0.3850\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.3545 - acc: 0.4131 - val_loss: 7.1204 - val_acc: 0.4010\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.8914 - acc: 0.4411 - val_loss: 6.6701 - val_acc: 0.4150\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.4494 - acc: 0.4615 - val_loss: 6.2386 - val_acc: 0.4880\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.0289 - acc: 0.4980 - val_loss: 5.8296 - val_acc: 0.5100\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 5.6285 - acc: 0.5261 - val_loss: 5.4421 - val_acc: 0.5080\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.2498 - acc: 0.5453 - val_loss: 5.0755 - val_acc: 0.5520\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.8939 - acc: 0.5620 - val_loss: 4.7329 - val_acc: 0.5670\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 4.5609 - acc: 0.5757 - val_loss: 4.4116 - val_acc: 0.5670\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.2494 - acc: 0.5856 - val_loss: 4.1126 - val_acc: 0.5920\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.9597 - acc: 0.6027 - val_loss: 3.8349 - val_acc: 0.5880\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.6924 - acc: 0.6088 - val_loss: 3.5796 - val_acc: 0.5940\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.4480 - acc: 0.6139 - val_loss: 3.3468 - val_acc: 0.6050\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.2246 - acc: 0.6192 - val_loss: 3.1336 - val_acc: 0.6110\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.0228 - acc: 0.6256 - val_loss: 2.9434 - val_acc: 0.6260\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.8426 - acc: 0.6315 - val_loss: 2.7743 - val_acc: 0.6130\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6843 - acc: 0.6337 - val_loss: 2.6267 - val_acc: 0.6430\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5475 - acc: 0.6429 - val_loss: 2.4985 - val_acc: 0.6360\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4305 - acc: 0.6460 - val_loss: 2.3923 - val_acc: 0.6320\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3326 - acc: 0.6492 - val_loss: 2.3054 - val_acc: 0.6230\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2540 - acc: 0.6487 - val_loss: 2.2348 - val_acc: 0.6560\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1923 - acc: 0.6535 - val_loss: 2.1796 - val_acc: 0.6530\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1465 - acc: 0.6576 - val_loss: 2.1410 - val_acc: 0.6630\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1120 - acc: 0.6609 - val_loss: 2.1102 - val_acc: 0.6600\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0836 - acc: 0.6639 - val_loss: 2.0808 - val_acc: 0.6670\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0583 - acc: 0.6659 - val_loss: 2.0566 - val_acc: 0.6690\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0353 - acc: 0.6657 - val_loss: 2.0351 - val_acc: 0.6720\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0132 - acc: 0.6695 - val_loss: 2.0149 - val_acc: 0.6590\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9929 - acc: 0.6700 - val_loss: 1.9950 - val_acc: 0.6790\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9732 - acc: 0.6739 - val_loss: 1.9783 - val_acc: 0.6760\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9548 - acc: 0.6759 - val_loss: 1.9549 - val_acc: 0.6820\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9362 - acc: 0.6780 - val_loss: 1.9387 - val_acc: 0.6800\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9195 - acc: 0.6768 - val_loss: 1.9231 - val_acc: 0.6800\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9028 - acc: 0.6791 - val_loss: 1.9045 - val_acc: 0.6850\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8874 - acc: 0.6799 - val_loss: 1.8885 - val_acc: 0.6840\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8716 - acc: 0.6813 - val_loss: 1.8737 - val_acc: 0.6850\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8572 - acc: 0.6840 - val_loss: 1.8594 - val_acc: 0.6830\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8426 - acc: 0.6844 - val_loss: 1.8457 - val_acc: 0.6940\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8291 - acc: 0.6861 - val_loss: 1.8313 - val_acc: 0.6880\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8154 - acc: 0.6871 - val_loss: 1.8208 - val_acc: 0.6920\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8029 - acc: 0.6869 - val_loss: 1.8053 - val_acc: 0.6900\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7905 - acc: 0.6872 - val_loss: 1.7935 - val_acc: 0.6950\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7784 - acc: 0.6897 - val_loss: 1.7809 - val_acc: 0.6950\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7664 - acc: 0.6904 - val_loss: 1.7694 - val_acc: 0.6970\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7551 - acc: 0.6904 - val_loss: 1.7584 - val_acc: 0.6980\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7435 - acc: 0.6903 - val_loss: 1.7492 - val_acc: 0.6950\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7332 - acc: 0.6909 - val_loss: 1.7370 - val_acc: 0.6940\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7223 - acc: 0.6924 - val_loss: 1.7265 - val_acc: 0.6980\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7117 - acc: 0.6925 - val_loss: 1.7177 - val_acc: 0.7030\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7022 - acc: 0.6924 - val_loss: 1.7052 - val_acc: 0.6950\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6918 - acc: 0.6940 - val_loss: 1.6960 - val_acc: 0.6950\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6822 - acc: 0.6937 - val_loss: 1.6867 - val_acc: 0.6920\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6727 - acc: 0.6945 - val_loss: 1.6761 - val_acc: 0.6950\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6630 - acc: 0.6957 - val_loss: 1.6671 - val_acc: 0.6950\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.6538 - acc: 0.6965 - val_loss: 1.6586 - val_acc: 0.6960\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6449 - acc: 0.6972 - val_loss: 1.6489 - val_acc: 0.6950\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6358 - acc: 0.6961 - val_loss: 1.6393 - val_acc: 0.6930\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6269 - acc: 0.6967 - val_loss: 1.6329 - val_acc: 0.7030\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6186 - acc: 0.6983 - val_loss: 1.6223 - val_acc: 0.6950\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6104 - acc: 0.6980 - val_loss: 1.6146 - val_acc: 0.6970\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6019 - acc: 0.6991 - val_loss: 1.6061 - val_acc: 0.6980\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5938 - acc: 0.7000 - val_loss: 1.5972 - val_acc: 0.6970\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5867 - acc: 0.7013 - val_loss: 1.5911 - val_acc: 0.6950\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5780 - acc: 0.7011 - val_loss: 1.5824 - val_acc: 0.6990\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5702 - acc: 0.7011 - val_loss: 1.5766 - val_acc: 0.6970\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5628 - acc: 0.7013 - val_loss: 1.5652 - val_acc: 0.6990\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5550 - acc: 0.7045 - val_loss: 1.5589 - val_acc: 0.6980\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5475 - acc: 0.7028 - val_loss: 1.5528 - val_acc: 0.7000\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5408 - acc: 0.7025 - val_loss: 1.5519 - val_acc: 0.6980\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5330 - acc: 0.7032 - val_loss: 1.5356 - val_acc: 0.7030\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5254 - acc: 0.7039 - val_loss: 1.5307 - val_acc: 0.7010\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5179 - acc: 0.7044 - val_loss: 1.5235 - val_acc: 0.7030\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5111 - acc: 0.7044 - val_loss: 1.5216 - val_acc: 0.7040\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7061 - val_loss: 1.5138 - val_acc: 0.7030\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4984 - acc: 0.7061 - val_loss: 1.5012 - val_acc: 0.7060\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4911 - acc: 0.7059 - val_loss: 1.4977 - val_acc: 0.7020\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4841 - acc: 0.7076 - val_loss: 1.4917 - val_acc: 0.7060\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4778 - acc: 0.7061 - val_loss: 1.4847 - val_acc: 0.7060\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4710 - acc: 0.7073 - val_loss: 1.4748 - val_acc: 0.7090\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4642 - acc: 0.7071 - val_loss: 1.4688 - val_acc: 0.7050\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4578 - acc: 0.7099 - val_loss: 1.4644 - val_acc: 0.7100\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4515 - acc: 0.7081 - val_loss: 1.4597 - val_acc: 0.7040\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4463 - acc: 0.7085 - val_loss: 1.4526 - val_acc: 0.7100\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4396 - acc: 0.7097 - val_loss: 1.4433 - val_acc: 0.7130\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4329 - acc: 0.7104 - val_loss: 1.4386 - val_acc: 0.7070\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4277 - acc: 0.7116 - val_loss: 1.4332 - val_acc: 0.7080\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4221 - acc: 0.7089 - val_loss: 1.4271 - val_acc: 0.7140\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4162 - acc: 0.7116 - val_loss: 1.4203 - val_acc: 0.7160\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4103 - acc: 0.7111 - val_loss: 1.4151 - val_acc: 0.7130\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4048 - acc: 0.7133 - val_loss: 1.4161 - val_acc: 0.7080\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3994 - acc: 0.7128 - val_loss: 1.4030 - val_acc: 0.7100\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3938 - acc: 0.7115 - val_loss: 1.3980 - val_acc: 0.7140\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3879 - acc: 0.7149 - val_loss: 1.3954 - val_acc: 0.7100\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3822 - acc: 0.7113 - val_loss: 1.3911 - val_acc: 0.7110\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3773 - acc: 0.7145 - val_loss: 1.3806 - val_acc: 0.7120\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3712 - acc: 0.7153 - val_loss: 1.3783 - val_acc: 0.7150\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3660 - acc: 0.7145 - val_loss: 1.3737 - val_acc: 0.7120\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3612 - acc: 0.7169 - val_loss: 1.3743 - val_acc: 0.7130\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3554 - acc: 0.7177 - val_loss: 1.3620 - val_acc: 0.7140\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3506 - acc: 0.7172 - val_loss: 1.3551 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3452 - acc: 0.7175 - val_loss: 1.3490 - val_acc: 0.7160\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3402 - acc: 0.7193 - val_loss: 1.3471 - val_acc: 0.7130\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3355 - acc: 0.7181 - val_loss: 1.3433 - val_acc: 0.7140\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3306 - acc: 0.7176 - val_loss: 1.3342 - val_acc: 0.7130\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3254 - acc: 0.7192 - val_loss: 1.3332 - val_acc: 0.7160\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3206 - acc: 0.7187 - val_loss: 1.3294 - val_acc: 0.7160\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3156 - acc: 0.7199 - val_loss: 1.3226 - val_acc: 0.7170\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0394 - acc: 0.1595 - val_loss: 15.6246 - val_acc: 0.1620\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 15.2714 - acc: 0.1924 - val_loss: 14.8786 - val_acc: 0.1870\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 14.5372 - acc: 0.2153 - val_loss: 14.1588 - val_acc: 0.2050\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 13.8259 - acc: 0.2327 - val_loss: 13.4607 - val_acc: 0.2170\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 13.1352 - acc: 0.2439 - val_loss: 12.7826 - val_acc: 0.2340\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 12.4647 - acc: 0.2531 - val_loss: 12.1247 - val_acc: 0.2490\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.8149 - acc: 0.2607 - val_loss: 11.4868 - val_acc: 0.2650\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 11.1858 - acc: 0.2709 - val_loss: 10.8691 - val_acc: 0.2780\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5773 - acc: 0.2869 - val_loss: 10.2715 - val_acc: 0.3020\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 9.9887 - acc: 0.3075 - val_loss: 9.6942 - val_acc: 0.3070\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 9.4198 - acc: 0.3132 - val_loss: 9.1365 - val_acc: 0.3240\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 8.8713 - acc: 0.3365 - val_loss: 8.6003 - val_acc: 0.3250\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 8.3445 - acc: 0.3568 - val_loss: 8.0852 - val_acc: 0.3840\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 7.8388 - acc: 0.3947 - val_loss: 7.5920 - val_acc: 0.3860\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.3544 - acc: 0.4127 - val_loss: 7.1203 - val_acc: 0.4020\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.8913 - acc: 0.4413 - val_loss: 6.6701 - val_acc: 0.4150\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.4493 - acc: 0.4617 - val_loss: 6.2385 - val_acc: 0.4880\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.0288 - acc: 0.4980 - val_loss: 5.8296 - val_acc: 0.5100\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 5.6285 - acc: 0.5263 - val_loss: 5.4419 - val_acc: 0.5090\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.2498 - acc: 0.5453 - val_loss: 5.0755 - val_acc: 0.5510\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.8938 - acc: 0.5623 - val_loss: 4.7329 - val_acc: 0.5650\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.5608 - acc: 0.5759 - val_loss: 4.4115 - val_acc: 0.5670\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 4.2493 - acc: 0.5861 - val_loss: 4.1126 - val_acc: 0.5930\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 3.9596 - acc: 0.6029 - val_loss: 3.8349 - val_acc: 0.5880\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.6923 - acc: 0.6087 - val_loss: 3.5795 - val_acc: 0.5940\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.4479 - acc: 0.6139 - val_loss: 3.3467 - val_acc: 0.6050\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.2245 - acc: 0.6195 - val_loss: 3.1336 - val_acc: 0.6110\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.0228 - acc: 0.6256 - val_loss: 2.9433 - val_acc: 0.6260\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.8425 - acc: 0.6316 - val_loss: 2.7743 - val_acc: 0.6120\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.6843 - acc: 0.6337 - val_loss: 2.6267 - val_acc: 0.6430\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5475 - acc: 0.6428 - val_loss: 2.4985 - val_acc: 0.6350\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.4304 - acc: 0.6460 - val_loss: 2.3923 - val_acc: 0.6320\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3325 - acc: 0.6491 - val_loss: 2.3054 - val_acc: 0.6230\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2540 - acc: 0.6487 - val_loss: 2.2348 - val_acc: 0.6570\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1923 - acc: 0.6535 - val_loss: 2.1796 - val_acc: 0.6530\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1465 - acc: 0.6572 - val_loss: 2.1411 - val_acc: 0.6630\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1120 - acc: 0.6607 - val_loss: 2.1103 - val_acc: 0.6590\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0836 - acc: 0.6639 - val_loss: 2.0808 - val_acc: 0.6670\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0582 - acc: 0.6657 - val_loss: 2.0566 - val_acc: 0.6690\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0353 - acc: 0.6657 - val_loss: 2.0351 - val_acc: 0.6710\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0132 - acc: 0.6697 - val_loss: 2.0148 - val_acc: 0.6600\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9930 - acc: 0.6700 - val_loss: 1.9950 - val_acc: 0.6790\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9733 - acc: 0.6740 - val_loss: 1.9783 - val_acc: 0.6760\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9548 - acc: 0.6757 - val_loss: 1.9550 - val_acc: 0.6820\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9362 - acc: 0.6780 - val_loss: 1.9387 - val_acc: 0.6800\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9195 - acc: 0.6769 - val_loss: 1.9230 - val_acc: 0.6810\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9028 - acc: 0.6793 - val_loss: 1.9046 - val_acc: 0.6850\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8874 - acc: 0.6799 - val_loss: 1.8886 - val_acc: 0.6840\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8716 - acc: 0.6813 - val_loss: 1.8736 - val_acc: 0.6850\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8572 - acc: 0.6840 - val_loss: 1.8594 - val_acc: 0.6830\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8426 - acc: 0.6845 - val_loss: 1.8457 - val_acc: 0.6940\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8291 - acc: 0.6857 - val_loss: 1.8313 - val_acc: 0.6880\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8154 - acc: 0.6871 - val_loss: 1.8209 - val_acc: 0.6920\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8029 - acc: 0.6869 - val_loss: 1.8054 - val_acc: 0.6900\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7906 - acc: 0.6872 - val_loss: 1.7936 - val_acc: 0.6940\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7784 - acc: 0.6897 - val_loss: 1.7809 - val_acc: 0.6950\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7665 - acc: 0.6905 - val_loss: 1.7695 - val_acc: 0.6970\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7552 - acc: 0.6905 - val_loss: 1.7585 - val_acc: 0.6980\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7436 - acc: 0.6904 - val_loss: 1.7492 - val_acc: 0.6950\n",
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7333 - acc: 0.6911 - val_loss: 1.7370 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7223 - acc: 0.6925 - val_loss: 1.7265 - val_acc: 0.6980\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7118 - acc: 0.6924 - val_loss: 1.7176 - val_acc: 0.7030\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7022 - acc: 0.6923 - val_loss: 1.7052 - val_acc: 0.6950\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6918 - acc: 0.6940 - val_loss: 1.6961 - val_acc: 0.6950\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6822 - acc: 0.6939 - val_loss: 1.6866 - val_acc: 0.6920\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6727 - acc: 0.6945 - val_loss: 1.6761 - val_acc: 0.6950\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6630 - acc: 0.6961 - val_loss: 1.6672 - val_acc: 0.6950\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6538 - acc: 0.6965 - val_loss: 1.6586 - val_acc: 0.6960\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6449 - acc: 0.6973 - val_loss: 1.6490 - val_acc: 0.6950\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6359 - acc: 0.6963 - val_loss: 1.6394 - val_acc: 0.6930\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6269 - acc: 0.6969 - val_loss: 1.6329 - val_acc: 0.7030\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6187 - acc: 0.6983 - val_loss: 1.6223 - val_acc: 0.6950\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6104 - acc: 0.6979 - val_loss: 1.6146 - val_acc: 0.6970\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6020 - acc: 0.6989 - val_loss: 1.6061 - val_acc: 0.6980\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5938 - acc: 0.7000 - val_loss: 1.5973 - val_acc: 0.6970\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5867 - acc: 0.7015 - val_loss: 1.5911 - val_acc: 0.6950\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5780 - acc: 0.7011 - val_loss: 1.5824 - val_acc: 0.6980\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5703 - acc: 0.7011 - val_loss: 1.5766 - val_acc: 0.6970\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5628 - acc: 0.7017 - val_loss: 1.5653 - val_acc: 0.6990\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5551 - acc: 0.7047 - val_loss: 1.5589 - val_acc: 0.6980\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5476 - acc: 0.7028 - val_loss: 1.5528 - val_acc: 0.7000\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5408 - acc: 0.7024 - val_loss: 1.5518 - val_acc: 0.6980\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5330 - acc: 0.7032 - val_loss: 1.5357 - val_acc: 0.7030\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5255 - acc: 0.7039 - val_loss: 1.5308 - val_acc: 0.7010\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5180 - acc: 0.7044 - val_loss: 1.5235 - val_acc: 0.7030\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5112 - acc: 0.7047 - val_loss: 1.5216 - val_acc: 0.7040\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7064 - val_loss: 1.5138 - val_acc: 0.7030\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4985 - acc: 0.7063 - val_loss: 1.5013 - val_acc: 0.7050\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7061 - val_loss: 1.4976 - val_acc: 0.7020\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4842 - acc: 0.7077 - val_loss: 1.4917 - val_acc: 0.7060\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4779 - acc: 0.7061 - val_loss: 1.4847 - val_acc: 0.7050\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4710 - acc: 0.7072 - val_loss: 1.4747 - val_acc: 0.7090\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4642 - acc: 0.7071 - val_loss: 1.4688 - val_acc: 0.7050\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4578 - acc: 0.7099 - val_loss: 1.4644 - val_acc: 0.7100\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4516 - acc: 0.7081 - val_loss: 1.4598 - val_acc: 0.7040\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4463 - acc: 0.7085 - val_loss: 1.4524 - val_acc: 0.7100\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4396 - acc: 0.7095 - val_loss: 1.4433 - val_acc: 0.7130\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4330 - acc: 0.7104 - val_loss: 1.4387 - val_acc: 0.7070\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4277 - acc: 0.7117 - val_loss: 1.4333 - val_acc: 0.7080\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4221 - acc: 0.7089 - val_loss: 1.4271 - val_acc: 0.7140\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4162 - acc: 0.7119 - val_loss: 1.4203 - val_acc: 0.7160\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4104 - acc: 0.7109 - val_loss: 1.4151 - val_acc: 0.7130\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4049 - acc: 0.7133 - val_loss: 1.4161 - val_acc: 0.7070\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3994 - acc: 0.7127 - val_loss: 1.4030 - val_acc: 0.7110\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3938 - acc: 0.7112 - val_loss: 1.3981 - val_acc: 0.7140\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3879 - acc: 0.7152 - val_loss: 1.3954 - val_acc: 0.7100\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3822 - acc: 0.7116 - val_loss: 1.3912 - val_acc: 0.7110\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3774 - acc: 0.7147 - val_loss: 1.3806 - val_acc: 0.7120\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3712 - acc: 0.7153 - val_loss: 1.3783 - val_acc: 0.7150\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3661 - acc: 0.7145 - val_loss: 1.3738 - val_acc: 0.7120\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3612 - acc: 0.7169 - val_loss: 1.3743 - val_acc: 0.7130\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3555 - acc: 0.7176 - val_loss: 1.3620 - val_acc: 0.7140\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3507 - acc: 0.7167 - val_loss: 1.3550 - val_acc: 0.7120\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3453 - acc: 0.7176 - val_loss: 1.3491 - val_acc: 0.7160\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3403 - acc: 0.7188 - val_loss: 1.3471 - val_acc: 0.7130\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3355 - acc: 0.7181 - val_loss: 1.3434 - val_acc: 0.7140\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3306 - acc: 0.7176 - val_loss: 1.3343 - val_acc: 0.7130\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3255 - acc: 0.7192 - val_loss: 1.3333 - val_acc: 0.7160\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3207 - acc: 0.7188 - val_loss: 1.3295 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3157 - acc: 0.7199 - val_loss: 1.3227 - val_acc: 0.7170\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3109 - acc: 0.7184 - val_loss: 1.3176 - val_acc: 0.7180\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3065 - acc: 0.7187 - val_loss: 1.3165 - val_acc: 0.7170\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3018 - acc: 0.7240 - val_loss: 1.3074 - val_acc: 0.7160\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2969 - acc: 0.7215 - val_loss: 1.3074 - val_acc: 0.7220\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2927 - acc: 0.7225 - val_loss: 1.2996 - val_acc: 0.7210\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2879 - acc: 0.7231 - val_loss: 1.2960 - val_acc: 0.7170\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2839 - acc: 0.7236 - val_loss: 1.2898 - val_acc: 0.7180\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2797 - acc: 0.7215 - val_loss: 1.2926 - val_acc: 0.7230\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2767 - acc: 0.7223 - val_loss: 1.2873 - val_acc: 0.7110\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2711 - acc: 0.7220 - val_loss: 1.2840 - val_acc: 0.7170\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2670 - acc: 0.7240 - val_loss: 1.2730 - val_acc: 0.7190\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2625 - acc: 0.7255 - val_loss: 1.2687 - val_acc: 0.7190\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2584 - acc: 0.7243 - val_loss: 1.2676 - val_acc: 0.7140\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2548 - acc: 0.7248 - val_loss: 1.2627 - val_acc: 0.7210\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2507 - acc: 0.7244 - val_loss: 1.2587 - val_acc: 0.7180\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2469 - acc: 0.7253 - val_loss: 1.2557 - val_acc: 0.7170\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2432 - acc: 0.7260 - val_loss: 1.2503 - val_acc: 0.7200\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2394 - acc: 0.7251 - val_loss: 1.2452 - val_acc: 0.7180\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2355 - acc: 0.7273 - val_loss: 1.2458 - val_acc: 0.7210\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2322 - acc: 0.7256 - val_loss: 1.2418 - val_acc: 0.7200\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2283 - acc: 0.7267 - val_loss: 1.2418 - val_acc: 0.7150\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2258 - acc: 0.7277 - val_loss: 1.2323 - val_acc: 0.7190\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2213 - acc: 0.7299 - val_loss: 1.2298 - val_acc: 0.7190\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2176 - acc: 0.7284 - val_loss: 1.2274 - val_acc: 0.7190\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2148 - acc: 0.7307 - val_loss: 1.2297 - val_acc: 0.7230\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2107 - acc: 0.7293 - val_loss: 1.2204 - val_acc: 0.7230\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2079 - acc: 0.7297 - val_loss: 1.2155 - val_acc: 0.7200\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2042 - acc: 0.7308 - val_loss: 1.2191 - val_acc: 0.7230\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2010 - acc: 0.7300 - val_loss: 1.2079 - val_acc: 0.7210\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1981 - acc: 0.7307 - val_loss: 1.2064 - val_acc: 0.7230\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1947 - acc: 0.7313 - val_loss: 1.2060 - val_acc: 0.7220\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1925 - acc: 0.7323 - val_loss: 1.2021 - val_acc: 0.7230\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1886 - acc: 0.7320 - val_loss: 1.2000 - val_acc: 0.7210\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1863 - acc: 0.7300 - val_loss: 1.2089 - val_acc: 0.7260\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1834 - acc: 0.7313 - val_loss: 1.1953 - val_acc: 0.7220\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1802 - acc: 0.7328 - val_loss: 1.1927 - val_acc: 0.7150\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1777 - acc: 0.7339 - val_loss: 1.1918 - val_acc: 0.7240\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1747 - acc: 0.7329 - val_loss: 1.1827 - val_acc: 0.7230\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1711 - acc: 0.7328 - val_loss: 1.1809 - val_acc: 0.7230\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1684 - acc: 0.7352 - val_loss: 1.1789 - val_acc: 0.7230\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1663 - acc: 0.7328 - val_loss: 1.1796 - val_acc: 0.7210\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1632 - acc: 0.7343 - val_loss: 1.1734 - val_acc: 0.7240\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1609 - acc: 0.7336 - val_loss: 1.1704 - val_acc: 0.7220\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7331 - val_loss: 1.1674 - val_acc: 0.7240\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1551 - acc: 0.7361 - val_loss: 1.1676 - val_acc: 0.7190\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1528 - acc: 0.7345 - val_loss: 1.1604 - val_acc: 0.7260\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1499 - acc: 0.7372 - val_loss: 1.1646 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1484 - acc: 0.7357 - val_loss: 1.1638 - val_acc: 0.7260\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1458 - acc: 0.7359 - val_loss: 1.1576 - val_acc: 0.7220\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1436 - acc: 0.7367 - val_loss: 1.1592 - val_acc: 0.7240\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1422 - acc: 0.7365 - val_loss: 1.1502 - val_acc: 0.7250\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1387 - acc: 0.7365 - val_loss: 1.1501 - val_acc: 0.7240\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1367 - acc: 0.7383 - val_loss: 1.1449 - val_acc: 0.7240\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1345 - acc: 0.7376 - val_loss: 1.1465 - val_acc: 0.7280\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1326 - acc: 0.7371 - val_loss: 1.1447 - val_acc: 0.7250\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1303 - acc: 0.7383 - val_loss: 1.1408 - val_acc: 0.7240\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1281 - acc: 0.7399 - val_loss: 1.1473 - val_acc: 0.7200\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1268 - acc: 0.7391 - val_loss: 1.1384 - val_acc: 0.7200\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1246 - acc: 0.7389 - val_loss: 1.1407 - val_acc: 0.7260\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1221 - acc: 0.7393 - val_loss: 1.1357 - val_acc: 0.7240\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1204 - acc: 0.7407 - val_loss: 1.1338 - val_acc: 0.7260\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1194 - acc: 0.7395 - val_loss: 1.1327 - val_acc: 0.7220\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1170 - acc: 0.7411 - val_loss: 1.1347 - val_acc: 0.7220\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1155 - acc: 0.7403 - val_loss: 1.1285 - val_acc: 0.7240\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1134 - acc: 0.7424 - val_loss: 1.1271 - val_acc: 0.7210\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1117 - acc: 0.7427 - val_loss: 1.1267 - val_acc: 0.7230\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1107 - acc: 0.7403 - val_loss: 1.1246 - val_acc: 0.7230\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1089 - acc: 0.7429 - val_loss: 1.1212 - val_acc: 0.7230\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1078 - acc: 0.7413 - val_loss: 1.1302 - val_acc: 0.7270\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1060 - acc: 0.7443 - val_loss: 1.1195 - val_acc: 0.7200\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1038 - acc: 0.7416 - val_loss: 1.1213 - val_acc: 0.7250\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1038 - acc: 0.7408 - val_loss: 1.1267 - val_acc: 0.7220\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1019 - acc: 0.7423 - val_loss: 1.1242 - val_acc: 0.7240\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1002 - acc: 0.7439 - val_loss: 1.1121 - val_acc: 0.7260\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0982 - acc: 0.7420 - val_loss: 1.1309 - val_acc: 0.7220\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0980 - acc: 0.7425 - val_loss: 1.1123 - val_acc: 0.7260\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0952 - acc: 0.7427 - val_loss: 1.1106 - val_acc: 0.7220\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0943 - acc: 0.7432 - val_loss: 1.1244 - val_acc: 0.7180\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0932 - acc: 0.7448 - val_loss: 1.1116 - val_acc: 0.7240\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0919 - acc: 0.7455 - val_loss: 1.1079 - val_acc: 0.7230\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0900 - acc: 0.7443 - val_loss: 1.1071 - val_acc: 0.7230\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0883 - acc: 0.7443 - val_loss: 1.1053 - val_acc: 0.7250\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0871 - acc: 0.7443 - val_loss: 1.1025 - val_acc: 0.7250\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0859 - acc: 0.7465 - val_loss: 1.1057 - val_acc: 0.7260\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0847 - acc: 0.7465 - val_loss: 1.0996 - val_acc: 0.7260\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0832 - acc: 0.7448 - val_loss: 1.1001 - val_acc: 0.7230\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0818 - acc: 0.7469 - val_loss: 1.0971 - val_acc: 0.7300\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0813 - acc: 0.7484 - val_loss: 1.0968 - val_acc: 0.7250\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0787 - acc: 0.7479 - val_loss: 1.1005 - val_acc: 0.7230\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0789 - acc: 0.7459 - val_loss: 1.0944 - val_acc: 0.7210\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0767 - acc: 0.7472 - val_loss: 1.0972 - val_acc: 0.7230\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0758 - acc: 0.7469 - val_loss: 1.0915 - val_acc: 0.7270\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0747 - acc: 0.7465 - val_loss: 1.0885 - val_acc: 0.7240\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0732 - acc: 0.7489 - val_loss: 1.0930 - val_acc: 0.7250\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0719 - acc: 0.7476 - val_loss: 1.0879 - val_acc: 0.7270\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0706 - acc: 0.7475 - val_loss: 1.0884 - val_acc: 0.7230\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0696 - acc: 0.7479 - val_loss: 1.0871 - val_acc: 0.7290\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0687 - acc: 0.7499 - val_loss: 1.0814 - val_acc: 0.7290\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0677 - acc: 0.7503 - val_loss: 1.0835 - val_acc: 0.7270\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0665 - acc: 0.7483 - val_loss: 1.0829 - val_acc: 0.7290\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0649 - acc: 0.7487 - val_loss: 1.0939 - val_acc: 0.7270\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0644 - acc: 0.7491 - val_loss: 1.0819 - val_acc: 0.7290\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0627 - acc: 0.7476 - val_loss: 1.0811 - val_acc: 0.7270\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0612 - acc: 0.7512 - val_loss: 1.0800 - val_acc: 0.7310\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0606 - acc: 0.7504 - val_loss: 1.0802 - val_acc: 0.7240\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0597 - acc: 0.7500 - val_loss: 1.0792 - val_acc: 0.7300\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0586 - acc: 0.7499 - val_loss: 1.0760 - val_acc: 0.7260\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0569 - acc: 0.7495 - val_loss: 1.0771 - val_acc: 0.7240\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0569 - acc: 0.7504 - val_loss: 1.0774 - val_acc: 0.7280\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0547 - acc: 0.7505 - val_loss: 1.0725 - val_acc: 0.7320\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0544 - acc: 0.7503 - val_loss: 1.0725 - val_acc: 0.7290\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0529 - acc: 0.7517 - val_loss: 1.0714 - val_acc: 0.7250\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0515 - acc: 0.7512 - val_loss: 1.0721 - val_acc: 0.7260\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0504 - acc: 0.7513 - val_loss: 1.0723 - val_acc: 0.7260\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0494 - acc: 0.7519 - val_loss: 1.0688 - val_acc: 0.7300\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0487 - acc: 0.7523 - val_loss: 1.0826 - val_acc: 0.7190\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0468 - acc: 0.7525 - val_loss: 1.0770 - val_acc: 0.7320\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0467 - acc: 0.7521 - val_loss: 1.0659 - val_acc: 0.7310\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0447 - acc: 0.7535 - val_loss: 1.0650 - val_acc: 0.7280\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0442 - acc: 0.7528 - val_loss: 1.0651 - val_acc: 0.7300\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0435 - acc: 0.7536 - val_loss: 1.0621 - val_acc: 0.7330\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0416 - acc: 0.7539 - val_loss: 1.0617 - val_acc: 0.7300\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0405 - acc: 0.7540 - val_loss: 1.0613 - val_acc: 0.7290\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0402 - acc: 0.7552 - val_loss: 1.0606 - val_acc: 0.7320\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0388 - acc: 0.7531 - val_loss: 1.0640 - val_acc: 0.7290\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0382 - acc: 0.7529 - val_loss: 1.0621 - val_acc: 0.7250\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0363 - acc: 0.7532 - val_loss: 1.0577 - val_acc: 0.7300\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0360 - acc: 0.7533 - val_loss: 1.0570 - val_acc: 0.7360\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0358 - acc: 0.7531 - val_loss: 1.0538 - val_acc: 0.7320\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0331 - acc: 0.7549 - val_loss: 1.0565 - val_acc: 0.7300\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0332 - acc: 0.7541 - val_loss: 1.0527 - val_acc: 0.7270\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0322 - acc: 0.7545 - val_loss: 1.0598 - val_acc: 0.7260\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0315 - acc: 0.7561 - val_loss: 1.0548 - val_acc: 0.7290\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0305 - acc: 0.7559 - val_loss: 1.0546 - val_acc: 0.7300\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0288 - acc: 0.7572 - val_loss: 1.0528 - val_acc: 0.7290\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0282 - acc: 0.7533 - val_loss: 1.0492 - val_acc: 0.7290\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0269 - acc: 0.7592 - val_loss: 1.0509 - val_acc: 0.7300\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0269 - acc: 0.7579 - val_loss: 1.0465 - val_acc: 0.7320\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0259 - acc: 0.7557 - val_loss: 1.0423 - val_acc: 0.7320\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0233 - acc: 0.7567 - val_loss: 1.0461 - val_acc: 0.7290\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0228 - acc: 0.7583 - val_loss: 1.0452 - val_acc: 0.7310\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0227 - acc: 0.7589 - val_loss: 1.0429 - val_acc: 0.7340\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0218 - acc: 0.7564 - val_loss: 1.0426 - val_acc: 0.7300\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0208 - acc: 0.7573 - val_loss: 1.0520 - val_acc: 0.7260\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0206 - acc: 0.7555 - val_loss: 1.0387 - val_acc: 0.7320\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0178 - acc: 0.7584 - val_loss: 1.0505 - val_acc: 0.7240\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0186 - acc: 0.7583 - val_loss: 1.0413 - val_acc: 0.7330\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0171 - acc: 0.7607 - val_loss: 1.0384 - val_acc: 0.7320\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0159 - acc: 0.7591 - val_loss: 1.0394 - val_acc: 0.7300\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0152 - acc: 0.7595 - val_loss: 1.0458 - val_acc: 0.7330\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0140 - acc: 0.7565 - val_loss: 1.0340 - val_acc: 0.7340\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0129 - acc: 0.7588 - val_loss: 1.0354 - val_acc: 0.7360\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0122 - acc: 0.7585 - val_loss: 1.0370 - val_acc: 0.7330\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0109 - acc: 0.7575 - val_loss: 1.0338 - val_acc: 0.7370\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0103 - acc: 0.7587 - val_loss: 1.0323 - val_acc: 0.7280\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0092 - acc: 0.7604 - val_loss: 1.0294 - val_acc: 0.7340\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0089 - acc: 0.7596 - val_loss: 1.0303 - val_acc: 0.7360\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0084 - acc: 0.7603 - val_loss: 1.0317 - val_acc: 0.7340\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0070 - acc: 0.7599 - val_loss: 1.0294 - val_acc: 0.7340\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0083 - acc: 0.7579 - val_loss: 1.0377 - val_acc: 0.7340\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0062 - acc: 0.7597 - val_loss: 1.0283 - val_acc: 0.7320\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0054 - acc: 0.7589 - val_loss: 1.0274 - val_acc: 0.7340\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7600 - val_loss: 1.0336 - val_acc: 0.7260\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0030 - acc: 0.7612 - val_loss: 1.0283 - val_acc: 0.7290\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0023 - acc: 0.7611 - val_loss: 1.0284 - val_acc: 0.7300\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0011 - acc: 0.7607 - val_loss: 1.0259 - val_acc: 0.7300\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0007 - acc: 0.7619 - val_loss: 1.0245 - val_acc: 0.7390\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0005 - acc: 0.7619 - val_loss: 1.0213 - val_acc: 0.7390\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9991 - acc: 0.7615 - val_loss: 1.0695 - val_acc: 0.7220\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9998 - acc: 0.7625 - val_loss: 1.0257 - val_acc: 0.7340\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9986 - acc: 0.7609 - val_loss: 1.0250 - val_acc: 0.7330\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9972 - acc: 0.7604 - val_loss: 1.0243 - val_acc: 0.7300\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9963 - acc: 0.7628 - val_loss: 1.0244 - val_acc: 0.7280\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9964 - acc: 0.7617 - val_loss: 1.0193 - val_acc: 0.7360\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9946 - acc: 0.7608 - val_loss: 1.0206 - val_acc: 0.7310\n",
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9938 - acc: 0.7607 - val_loss: 1.0249 - val_acc: 0.7330\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9944 - acc: 0.7616 - val_loss: 1.0213 - val_acc: 0.7390\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9932 - acc: 0.7628 - val_loss: 1.0207 - val_acc: 0.7330\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9918 - acc: 0.7643 - val_loss: 1.0213 - val_acc: 0.7360\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9919 - acc: 0.7625 - val_loss: 1.0153 - val_acc: 0.7340\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9913 - acc: 0.7635 - val_loss: 1.0236 - val_acc: 0.7360\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9894 - acc: 0.7652 - val_loss: 1.0179 - val_acc: 0.7380\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9890 - acc: 0.7633 - val_loss: 1.0149 - val_acc: 0.7370\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9889 - acc: 0.7636 - val_loss: 1.0175 - val_acc: 0.7360\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9874 - acc: 0.7637 - val_loss: 1.0133 - val_acc: 0.7330\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9877 - acc: 0.7629 - val_loss: 1.0193 - val_acc: 0.7370\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9865 - acc: 0.7635 - val_loss: 1.0138 - val_acc: 0.7380\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9869 - acc: 0.7637 - val_loss: 1.0158 - val_acc: 0.7310\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9860 - acc: 0.7629 - val_loss: 1.0213 - val_acc: 0.7270\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9857 - acc: 0.7624 - val_loss: 1.0150 - val_acc: 0.7300\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9832 - acc: 0.7631 - val_loss: 1.0093 - val_acc: 0.7350\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9830 - acc: 0.7640 - val_loss: 1.0127 - val_acc: 0.7310\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9836 - acc: 0.7615 - val_loss: 1.0141 - val_acc: 0.7320\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9822 - acc: 0.7644 - val_loss: 1.0089 - val_acc: 0.7380\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9824 - acc: 0.7629 - val_loss: 1.0152 - val_acc: 0.7390\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9804 - acc: 0.7647 - val_loss: 1.0118 - val_acc: 0.7370\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9799 - acc: 0.7655 - val_loss: 1.0047 - val_acc: 0.7390\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9793 - acc: 0.7627 - val_loss: 1.0041 - val_acc: 0.7390\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9793 - acc: 0.7653 - val_loss: 1.0073 - val_acc: 0.7340\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9786 - acc: 0.7643 - val_loss: 1.0073 - val_acc: 0.7380\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9776 - acc: 0.7661 - val_loss: 1.0041 - val_acc: 0.7350\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9773 - acc: 0.7643 - val_loss: 1.0029 - val_acc: 0.7410\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9763 - acc: 0.7652 - val_loss: 1.0040 - val_acc: 0.7370\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9759 - acc: 0.7640 - val_loss: 1.0050 - val_acc: 0.7370\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9744 - acc: 0.7668 - val_loss: 1.0031 - val_acc: 0.7350\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9736 - acc: 0.7672 - val_loss: 1.0092 - val_acc: 0.7340\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9749 - acc: 0.7665 - val_loss: 0.9997 - val_acc: 0.7380\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9740 - acc: 0.7664 - val_loss: 1.0015 - val_acc: 0.7410\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9733 - acc: 0.7657 - val_loss: 1.0060 - val_acc: 0.7380\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9727 - acc: 0.7663 - val_loss: 1.0045 - val_acc: 0.7370\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9721 - acc: 0.7661 - val_loss: 1.0027 - val_acc: 0.7390\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9725 - acc: 0.7661 - val_loss: 1.0063 - val_acc: 0.7310\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9714 - acc: 0.7668 - val_loss: 1.0000 - val_acc: 0.7400\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9701 - acc: 0.7667 - val_loss: 1.0008 - val_acc: 0.7390\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9698 - acc: 0.7672 - val_loss: 1.0004 - val_acc: 0.7360\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9692 - acc: 0.7681 - val_loss: 0.9990 - val_acc: 0.7380\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9695 - acc: 0.7683 - val_loss: 1.0047 - val_acc: 0.7410\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9676 - acc: 0.7685 - val_loss: 0.9960 - val_acc: 0.7420\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9682 - acc: 0.7664 - val_loss: 0.9935 - val_acc: 0.7410\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9664 - acc: 0.7676 - val_loss: 1.0041 - val_acc: 0.7320\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9670 - acc: 0.7665 - val_loss: 0.9932 - val_acc: 0.7410\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9656 - acc: 0.7667 - val_loss: 0.9976 - val_acc: 0.7390\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9653 - acc: 0.7668 - val_loss: 0.9991 - val_acc: 0.7380\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9943 - val_acc: 0.7350\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9639 - acc: 0.7688 - val_loss: 0.9957 - val_acc: 0.7410\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9636 - acc: 0.7684 - val_loss: 1.0064 - val_acc: 0.7330\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9649 - acc: 0.7672 - val_loss: 0.9987 - val_acc: 0.7390\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9625 - acc: 0.7692 - val_loss: 0.9953 - val_acc: 0.7350\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9619 - acc: 0.7701 - val_loss: 0.9987 - val_acc: 0.7420\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9618 - acc: 0.7675 - val_loss: 1.0001 - val_acc: 0.7340\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9607 - acc: 0.7677 - val_loss: 0.9979 - val_acc: 0.7360\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9606 - acc: 0.7688 - val_loss: 0.9894 - val_acc: 0.7360\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9592 - acc: 0.7716 - val_loss: 0.9972 - val_acc: 0.7380\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9586 - acc: 0.7692 - val_loss: 0.9862 - val_acc: 0.7470\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9593 - acc: 0.7681 - val_loss: 0.9978 - val_acc: 0.7350\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9584 - acc: 0.7679 - val_loss: 0.9971 - val_acc: 0.7370\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9581 - acc: 0.7687 - val_loss: 0.9958 - val_acc: 0.7350\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9576 - acc: 0.7688 - val_loss: 0.9927 - val_acc: 0.7360\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9571 - acc: 0.7685 - val_loss: 0.9919 - val_acc: 0.7380\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9557 - acc: 0.7693 - val_loss: 0.9850 - val_acc: 0.7380\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9540 - acc: 0.7707 - val_loss: 0.9888 - val_acc: 0.7390\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9547 - acc: 0.7715 - val_loss: 0.9859 - val_acc: 0.7400\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9544 - acc: 0.7700 - val_loss: 0.9835 - val_acc: 0.7440\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9551 - acc: 0.7712 - val_loss: 0.9834 - val_acc: 0.7450\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9535 - acc: 0.7693 - val_loss: 0.9856 - val_acc: 0.7440\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9545 - acc: 0.7704 - val_loss: 0.9842 - val_acc: 0.7430\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9524 - acc: 0.7707 - val_loss: 0.9903 - val_acc: 0.7360\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9520 - acc: 0.7705 - val_loss: 0.9952 - val_acc: 0.7320\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9525 - acc: 0.7695 - val_loss: 0.9933 - val_acc: 0.7430\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9510 - acc: 0.7695 - val_loss: 0.9962 - val_acc: 0.7370\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9515 - acc: 0.7725 - val_loss: 0.9900 - val_acc: 0.7450\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9508 - acc: 0.7699 - val_loss: 0.9889 - val_acc: 0.7430\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9502 - acc: 0.7723 - val_loss: 0.9937 - val_acc: 0.7430\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9499 - acc: 0.7713 - val_loss: 0.9890 - val_acc: 0.7360\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9491 - acc: 0.7708 - val_loss: 0.9835 - val_acc: 0.7390\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9500 - acc: 0.7711 - val_loss: 0.9890 - val_acc: 0.7340\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9471 - acc: 0.7721 - val_loss: 1.0021 - val_acc: 0.7390\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9494 - acc: 0.7679 - val_loss: 0.9823 - val_acc: 0.7440\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9472 - acc: 0.7708 - val_loss: 0.9854 - val_acc: 0.7440\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9461 - acc: 0.7721 - val_loss: 0.9781 - val_acc: 0.7390\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9469 - acc: 0.7733 - val_loss: 0.9826 - val_acc: 0.7430\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9462 - acc: 0.7715 - val_loss: 0.9781 - val_acc: 0.7400\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9447 - acc: 0.7715 - val_loss: 0.9814 - val_acc: 0.7400\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9442 - acc: 0.7724 - val_loss: 0.9779 - val_acc: 0.7410\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9433 - acc: 0.7712 - val_loss: 0.9759 - val_acc: 0.7410\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9437 - acc: 0.7720 - val_loss: 0.9779 - val_acc: 0.7440\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9449 - acc: 0.7715 - val_loss: 0.9793 - val_acc: 0.7440\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9428 - acc: 0.7724 - val_loss: 0.9880 - val_acc: 0.7370\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9434 - acc: 0.7719 - val_loss: 0.9821 - val_acc: 0.7460\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9421 - acc: 0.7724 - val_loss: 0.9812 - val_acc: 0.7460\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9426 - acc: 0.7708 - val_loss: 0.9830 - val_acc: 0.7500\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9419 - acc: 0.7735 - val_loss: 0.9790 - val_acc: 0.7400\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9423 - acc: 0.7713 - val_loss: 0.9784 - val_acc: 0.7350\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9408 - acc: 0.7712 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9404 - acc: 0.7747 - val_loss: 0.9754 - val_acc: 0.7400\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9390 - acc: 0.7743 - val_loss: 0.9801 - val_acc: 0.7450\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9397 - acc: 0.7736 - val_loss: 0.9749 - val_acc: 0.7380\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9380 - acc: 0.7740 - val_loss: 0.9773 - val_acc: 0.7420\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9377 - acc: 0.7739 - val_loss: 0.9858 - val_acc: 0.7380\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9395 - acc: 0.7731 - val_loss: 0.9712 - val_acc: 0.7450\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9379 - acc: 0.7731 - val_loss: 0.9731 - val_acc: 0.7480\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9368 - acc: 0.7741 - val_loss: 0.9730 - val_acc: 0.7480\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9367 - acc: 0.7740 - val_loss: 0.9815 - val_acc: 0.7400\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9367 - acc: 0.7719 - val_loss: 0.9744 - val_acc: 0.7450\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9366 - acc: 0.7749 - val_loss: 0.9803 - val_acc: 0.7320\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9344 - acc: 0.7749 - val_loss: 0.9715 - val_acc: 0.7440\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9361 - acc: 0.7735 - val_loss: 0.9795 - val_acc: 0.7370\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9353 - acc: 0.7711 - val_loss: 0.9738 - val_acc: 0.7390\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9342 - acc: 0.7747 - val_loss: 0.9822 - val_acc: 0.7400\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9349 - acc: 0.7735 - val_loss: 0.9876 - val_acc: 0.7380\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9333 - acc: 0.7743 - val_loss: 0.9952 - val_acc: 0.7430\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9339 - acc: 0.7735 - val_loss: 0.9705 - val_acc: 0.7390\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9333 - acc: 0.7727 - val_loss: 0.9674 - val_acc: 0.7420\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9330 - acc: 0.7728 - val_loss: 0.9783 - val_acc: 0.7450\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9320 - acc: 0.7739 - val_loss: 0.9688 - val_acc: 0.7460\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9310 - acc: 0.7745 - val_loss: 0.9743 - val_acc: 0.7510\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9306 - acc: 0.7772 - val_loss: 0.9740 - val_acc: 0.7420\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9304 - acc: 0.7739 - val_loss: 0.9703 - val_acc: 0.7410\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9307 - acc: 0.7753 - val_loss: 0.9725 - val_acc: 0.7460\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9298 - acc: 0.7751 - val_loss: 0.9860 - val_acc: 0.7430\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9299 - acc: 0.7735 - val_loss: 0.9698 - val_acc: 0.7470\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9297 - acc: 0.7747 - val_loss: 0.9662 - val_acc: 0.7480\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9290 - acc: 0.7765 - val_loss: 0.9644 - val_acc: 0.7460\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9274 - acc: 0.7728 - val_loss: 0.9747 - val_acc: 0.7450\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9283 - acc: 0.7731 - val_loss: 0.9636 - val_acc: 0.7400\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9268 - acc: 0.7749 - val_loss: 0.9746 - val_acc: 0.7360\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9274 - acc: 0.7760 - val_loss: 0.9679 - val_acc: 0.7500\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9261 - acc: 0.7771 - val_loss: 0.9666 - val_acc: 0.7530\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9255 - acc: 0.7761 - val_loss: 0.9666 - val_acc: 0.7450\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9259 - acc: 0.7756 - val_loss: 0.9690 - val_acc: 0.7420\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9254 - acc: 0.7776 - val_loss: 0.9642 - val_acc: 0.7440\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9243 - acc: 0.7763 - val_loss: 0.9637 - val_acc: 0.7450\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9252 - acc: 0.7753 - val_loss: 0.9695 - val_acc: 0.7460\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9252 - acc: 0.7757 - val_loss: 0.9640 - val_acc: 0.7390\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9235 - acc: 0.7765 - val_loss: 0.9667 - val_acc: 0.7520\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9244 - acc: 0.7760 - val_loss: 0.9643 - val_acc: 0.7480\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9230 - acc: 0.7771 - val_loss: 0.9667 - val_acc: 0.7480\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9229 - acc: 0.7760 - val_loss: 0.9725 - val_acc: 0.7360\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9224 - acc: 0.7773 - val_loss: 0.9671 - val_acc: 0.7430\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9234 - acc: 0.7768 - val_loss: 0.9592 - val_acc: 0.7500\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9212 - acc: 0.7773 - val_loss: 0.9973 - val_acc: 0.7400\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9220 - acc: 0.7776 - val_loss: 0.9612 - val_acc: 0.7460\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9215 - acc: 0.7783 - val_loss: 0.9712 - val_acc: 0.7520\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9209 - acc: 0.7747 - val_loss: 0.9581 - val_acc: 0.7480\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9199 - acc: 0.7783 - val_loss: 0.9632 - val_acc: 0.7450\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9201 - acc: 0.7772 - val_loss: 0.9665 - val_acc: 0.7500\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9189 - acc: 0.7771 - val_loss: 0.9775 - val_acc: 0.7470\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9192 - acc: 0.7777 - val_loss: 0.9576 - val_acc: 0.7500\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9190 - acc: 0.7775 - val_loss: 0.9557 - val_acc: 0.7490\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9189 - acc: 0.7759 - val_loss: 0.9555 - val_acc: 0.7490\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9167 - acc: 0.7776 - val_loss: 0.9662 - val_acc: 0.7350\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9185 - acc: 0.7756 - val_loss: 0.9608 - val_acc: 0.7440\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9176 - acc: 0.7792 - val_loss: 0.9547 - val_acc: 0.7440\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9175 - acc: 0.7785 - val_loss: 0.9546 - val_acc: 0.7480\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9176 - acc: 0.7784 - val_loss: 0.9571 - val_acc: 0.7500\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9159 - acc: 0.7769 - val_loss: 0.9630 - val_acc: 0.7450\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9162 - acc: 0.7783 - val_loss: 0.9537 - val_acc: 0.7460\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7804 - val_loss: 0.9565 - val_acc: 0.7480\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9162 - acc: 0.7775 - val_loss: 0.9566 - val_acc: 0.7550\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9147 - acc: 0.7779 - val_loss: 0.9547 - val_acc: 0.7440\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9144 - acc: 0.7791 - val_loss: 0.9556 - val_acc: 0.7510\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9139 - acc: 0.7781 - val_loss: 0.9781 - val_acc: 0.7460\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9155 - acc: 0.7772 - val_loss: 0.9631 - val_acc: 0.7500\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9133 - acc: 0.7788 - val_loss: 0.9579 - val_acc: 0.7430\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9129 - acc: 0.7800 - val_loss: 0.9579 - val_acc: 0.7490\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9121 - acc: 0.7801 - val_loss: 0.9554 - val_acc: 0.7410\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9125 - acc: 0.7801 - val_loss: 0.9555 - val_acc: 0.7550\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9129 - acc: 0.7805 - val_loss: 0.9511 - val_acc: 0.7510\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9109 - acc: 0.7799 - val_loss: 0.9580 - val_acc: 0.7470\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9116 - acc: 0.7795 - val_loss: 0.9543 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9115 - acc: 0.7779 - val_loss: 0.9553 - val_acc: 0.7440\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9114 - acc: 0.7801 - val_loss: 0.9584 - val_acc: 0.7480\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9108 - acc: 0.7795 - val_loss: 0.9513 - val_acc: 0.7420\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9100 - acc: 0.7793 - val_loss: 0.9536 - val_acc: 0.7540\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9100 - acc: 0.7797 - val_loss: 0.9534 - val_acc: 0.7460\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9088 - acc: 0.7788 - val_loss: 0.9613 - val_acc: 0.7490\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9103 - acc: 0.7799 - val_loss: 0.9544 - val_acc: 0.7440\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9092 - acc: 0.7795 - val_loss: 0.9625 - val_acc: 0.7470\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9090 - acc: 0.7795 - val_loss: 0.9490 - val_acc: 0.7530\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9071 - acc: 0.7805 - val_loss: 0.9668 - val_acc: 0.7360\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9081 - acc: 0.7787 - val_loss: 0.9571 - val_acc: 0.7510\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9067 - acc: 0.7811 - val_loss: 0.9616 - val_acc: 0.7490\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9085 - acc: 0.7799 - val_loss: 0.9543 - val_acc: 0.7540\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9073 - acc: 0.7785 - val_loss: 0.9564 - val_acc: 0.7460\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9071 - acc: 0.7780 - val_loss: 0.9485 - val_acc: 0.7470\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9067 - acc: 0.7801 - val_loss: 0.9530 - val_acc: 0.7530\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9043 - acc: 0.7820 - val_loss: 0.9581 - val_acc: 0.7510\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9053 - acc: 0.7815 - val_loss: 0.9492 - val_acc: 0.7460\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9045 - acc: 0.7808 - val_loss: 0.9479 - val_acc: 0.7570\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9037 - acc: 0.7807 - val_loss: 0.9595 - val_acc: 0.7350\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9037 - acc: 0.7823 - val_loss: 0.9670 - val_acc: 0.7470\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9050 - acc: 0.7839 - val_loss: 0.9581 - val_acc: 0.7420\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9038 - acc: 0.7792 - val_loss: 0.9481 - val_acc: 0.7500\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9045 - acc: 0.7780 - val_loss: 0.9463 - val_acc: 0.7550\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9045 - acc: 0.7815 - val_loss: 0.9450 - val_acc: 0.7530\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9033 - acc: 0.7828 - val_loss: 0.9498 - val_acc: 0.7570\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9021 - acc: 0.7812 - val_loss: 0.9443 - val_acc: 0.7490\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9025 - acc: 0.7823 - val_loss: 0.9556 - val_acc: 0.7440\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9013 - acc: 0.7817 - val_loss: 0.9502 - val_acc: 0.7510\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9023 - acc: 0.7801 - val_loss: 0.9493 - val_acc: 0.7530\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9015 - acc: 0.7805 - val_loss: 0.9546 - val_acc: 0.7570\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9007 - acc: 0.7817 - val_loss: 0.9515 - val_acc: 0.7410\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9027 - acc: 0.7816 - val_loss: 0.9491 - val_acc: 0.7530\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9000 - acc: 0.7817 - val_loss: 0.9437 - val_acc: 0.7510\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9004 - acc: 0.7821 - val_loss: 0.9449 - val_acc: 0.7500\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7797 - val_loss: 0.9493 - val_acc: 0.7570\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8995 - acc: 0.7821 - val_loss: 0.9598 - val_acc: 0.7450\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9001 - acc: 0.7804 - val_loss: 0.9434 - val_acc: 0.7550\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8976 - acc: 0.7811 - val_loss: 0.9451 - val_acc: 0.7520\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8979 - acc: 0.7805 - val_loss: 0.9587 - val_acc: 0.7480\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8986 - acc: 0.7828 - val_loss: 0.9526 - val_acc: 0.7500\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8983 - acc: 0.7817 - val_loss: 0.9474 - val_acc: 0.7570\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8967 - acc: 0.7805 - val_loss: 0.9411 - val_acc: 0.7540\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8976 - acc: 0.7828 - val_loss: 0.9520 - val_acc: 0.7440\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8980 - acc: 0.7813 - val_loss: 0.9511 - val_acc: 0.7580\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8973 - acc: 0.7825 - val_loss: 0.9625 - val_acc: 0.7420\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8977 - acc: 0.7825 - val_loss: 0.9453 - val_acc: 0.7470\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8945 - acc: 0.7824 - val_loss: 0.9466 - val_acc: 0.7440\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8951 - acc: 0.7811 - val_loss: 0.9445 - val_acc: 0.7540\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8960 - acc: 0.7833 - val_loss: 0.9505 - val_acc: 0.7500\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8956 - acc: 0.7807 - val_loss: 0.9434 - val_acc: 0.7580\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8946 - acc: 0.7821 - val_loss: 0.9434 - val_acc: 0.7540\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8943 - acc: 0.7841 - val_loss: 0.9407 - val_acc: 0.7590\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8929 - acc: 0.7831 - val_loss: 0.9396 - val_acc: 0.7550\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8944 - acc: 0.7820 - val_loss: 0.9588 - val_acc: 0.7580\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8951 - acc: 0.7812 - val_loss: 0.9376 - val_acc: 0.7560\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8924 - acc: 0.7849 - val_loss: 0.9425 - val_acc: 0.7510\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8924 - acc: 0.7847 - val_loss: 0.9414 - val_acc: 0.7560\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8920 - acc: 0.7849 - val_loss: 0.9475 - val_acc: 0.7440\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8930 - acc: 0.7845 - val_loss: 0.9382 - val_acc: 0.7520\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8915 - acc: 0.7809 - val_loss: 0.9395 - val_acc: 0.7580\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8930 - acc: 0.7824 - val_loss: 0.9424 - val_acc: 0.7500\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8926 - acc: 0.7852 - val_loss: 0.9421 - val_acc: 0.7590\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8905 - acc: 0.7840 - val_loss: 0.9364 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8896 - acc: 0.7828 - val_loss: 0.9463 - val_acc: 0.7510\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8892 - acc: 0.7837 - val_loss: 0.9391 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8910 - acc: 0.7829 - val_loss: 0.9398 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8893 - acc: 0.7823 - val_loss: 0.9348 - val_acc: 0.7540\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8892 - acc: 0.7824 - val_loss: 0.9351 - val_acc: 0.7520\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8893 - acc: 0.7840 - val_loss: 0.9387 - val_acc: 0.7550\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8889 - acc: 0.7832 - val_loss: 0.9395 - val_acc: 0.7540\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8868 - acc: 0.7832 - val_loss: 0.9374 - val_acc: 0.7590\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8889 - acc: 0.7825 - val_loss: 0.9474 - val_acc: 0.7560\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8884 - acc: 0.7812 - val_loss: 0.9352 - val_acc: 0.7530\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8881 - acc: 0.7833 - val_loss: 0.9453 - val_acc: 0.7510\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8873 - acc: 0.7841 - val_loss: 0.9353 - val_acc: 0.7600\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8888 - acc: 0.7833 - val_loss: 0.9372 - val_acc: 0.7480\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8859 - acc: 0.7836 - val_loss: 0.9429 - val_acc: 0.7490\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8863 - acc: 0.7852 - val_loss: 0.9371 - val_acc: 0.7460\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8882 - acc: 0.7812 - val_loss: 0.9359 - val_acc: 0.7560\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8862 - acc: 0.7849 - val_loss: 0.9406 - val_acc: 0.7470\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8881 - acc: 0.7840 - val_loss: 0.9469 - val_acc: 0.7530\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8868 - acc: 0.7843 - val_loss: 0.9387 - val_acc: 0.7520\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8860 - acc: 0.7840 - val_loss: 0.9414 - val_acc: 0.7630\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8849 - acc: 0.7836 - val_loss: 0.9316 - val_acc: 0.7580\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8868 - acc: 0.7824 - val_loss: 0.9342 - val_acc: 0.7500\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8847 - acc: 0.7852 - val_loss: 0.9367 - val_acc: 0.7460\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8842 - acc: 0.7841 - val_loss: 0.9318 - val_acc: 0.7540\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8841 - acc: 0.7831 - val_loss: 0.9443 - val_acc: 0.7430\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8841 - acc: 0.7825 - val_loss: 0.9327 - val_acc: 0.7610\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8850 - acc: 0.7833 - val_loss: 0.9415 - val_acc: 0.7450\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8852 - acc: 0.7832 - val_loss: 0.9347 - val_acc: 0.7540\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8815 - acc: 0.7844 - val_loss: 0.9371 - val_acc: 0.7600\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8824 - acc: 0.7857 - val_loss: 0.9411 - val_acc: 0.7520\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8822 - acc: 0.7861 - val_loss: 0.9556 - val_acc: 0.7530\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8821 - acc: 0.7871 - val_loss: 0.9353 - val_acc: 0.7530\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8825 - acc: 0.7847 - val_loss: 0.9341 - val_acc: 0.7560\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8824 - acc: 0.7819 - val_loss: 0.9330 - val_acc: 0.7470\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8825 - acc: 0.7853 - val_loss: 0.9319 - val_acc: 0.7490\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8798 - acc: 0.7857 - val_loss: 0.9440 - val_acc: 0.7550\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8813 - acc: 0.7871 - val_loss: 0.9319 - val_acc: 0.7500\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8803 - acc: 0.7852 - val_loss: 0.9437 - val_acc: 0.7420\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8820 - acc: 0.7861 - val_loss: 0.9365 - val_acc: 0.7530\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8804 - acc: 0.7860 - val_loss: 0.9434 - val_acc: 0.7550\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8809 - acc: 0.7831 - val_loss: 0.9289 - val_acc: 0.7590\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8800 - acc: 0.7853 - val_loss: 0.9379 - val_acc: 0.7500\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8797 - acc: 0.7852 - val_loss: 0.9706 - val_acc: 0.7460\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8814 - acc: 0.7847 - val_loss: 0.9408 - val_acc: 0.7510\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8808 - acc: 0.7876 - val_loss: 0.9353 - val_acc: 0.7530\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8784 - acc: 0.7844 - val_loss: 0.9294 - val_acc: 0.7600\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8790 - acc: 0.7843 - val_loss: 0.9317 - val_acc: 0.7540\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8791 - acc: 0.7863 - val_loss: 0.9470 - val_acc: 0.7580\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8799 - acc: 0.7864 - val_loss: 0.9372 - val_acc: 0.7520\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8787 - acc: 0.7851 - val_loss: 0.9490 - val_acc: 0.7510\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8787 - acc: 0.7849 - val_loss: 0.9294 - val_acc: 0.7580\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8776 - acc: 0.7863 - val_loss: 0.9296 - val_acc: 0.7540\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8768 - acc: 0.7860 - val_loss: 0.9293 - val_acc: 0.7490\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8781 - acc: 0.7848 - val_loss: 0.9344 - val_acc: 0.7580\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8772 - acc: 0.7853 - val_loss: 0.9331 - val_acc: 0.7530\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8764 - acc: 0.7849 - val_loss: 0.9540 - val_acc: 0.7410\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8782 - acc: 0.7855 - val_loss: 0.9358 - val_acc: 0.7430\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8766 - acc: 0.7849 - val_loss: 0.9269 - val_acc: 0.7560\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8755 - acc: 0.7869 - val_loss: 0.9286 - val_acc: 0.7550\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8773 - acc: 0.7852 - val_loss: 0.9318 - val_acc: 0.7530\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8776 - acc: 0.7845 - val_loss: 0.9301 - val_acc: 0.7520\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8752 - acc: 0.7872 - val_loss: 0.9334 - val_acc: 0.7480\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8748 - acc: 0.7861 - val_loss: 0.9767 - val_acc: 0.7400\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8756 - acc: 0.7885 - val_loss: 0.9270 - val_acc: 0.7470\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8757 - acc: 0.7867 - val_loss: 0.9294 - val_acc: 0.7630\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8755 - acc: 0.7868 - val_loss: 0.9276 - val_acc: 0.7450\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8755 - acc: 0.7860 - val_loss: 0.9333 - val_acc: 0.7450\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8750 - acc: 0.7871 - val_loss: 0.9290 - val_acc: 0.7580\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8744 - acc: 0.7892 - val_loss: 0.9301 - val_acc: 0.7570\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8737 - acc: 0.7876 - val_loss: 0.9608 - val_acc: 0.7520\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8746 - acc: 0.7861 - val_loss: 0.9258 - val_acc: 0.7550\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8734 - acc: 0.7873 - val_loss: 0.9251 - val_acc: 0.7600\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8757 - acc: 0.7849 - val_loss: 0.9264 - val_acc: 0.7520\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8738 - acc: 0.7859 - val_loss: 0.9248 - val_acc: 0.7560\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8722 - acc: 0.7861 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8729 - acc: 0.7871 - val_loss: 0.9532 - val_acc: 0.7460\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8752 - acc: 0.7873 - val_loss: 0.9526 - val_acc: 0.7450\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8747 - acc: 0.7860 - val_loss: 0.9338 - val_acc: 0.7460\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8720 - acc: 0.7855 - val_loss: 0.9557 - val_acc: 0.7400\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8727 - acc: 0.7904 - val_loss: 0.9381 - val_acc: 0.7530\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8731 - acc: 0.7869 - val_loss: 0.9282 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8710 - acc: 0.7889 - val_loss: 0.9272 - val_acc: 0.7570\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8727 - acc: 0.7877 - val_loss: 0.9316 - val_acc: 0.7550\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8730 - acc: 0.7863 - val_loss: 0.9309 - val_acc: 0.7550\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8716 - acc: 0.7863 - val_loss: 0.9858 - val_acc: 0.7440\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8732 - acc: 0.7861 - val_loss: 0.9599 - val_acc: 0.7430\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8714 - acc: 0.7879 - val_loss: 0.9237 - val_acc: 0.7570\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8703 - acc: 0.7852 - val_loss: 0.9290 - val_acc: 0.7560\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8697 - acc: 0.7872 - val_loss: 0.9788 - val_acc: 0.7390\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8751 - acc: 0.7848 - val_loss: 0.9250 - val_acc: 0.7540\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8698 - acc: 0.7879 - val_loss: 0.9324 - val_acc: 0.7560\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8718 - acc: 0.7840 - val_loss: 0.9424 - val_acc: 0.7640\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8704 - acc: 0.7873 - val_loss: 0.9315 - val_acc: 0.7520\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8700 - acc: 0.7873 - val_loss: 0.9263 - val_acc: 0.7510\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8684 - acc: 0.7900 - val_loss: 0.9268 - val_acc: 0.7500\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8691 - acc: 0.7875 - val_loss: 0.9290 - val_acc: 0.7560\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8685 - acc: 0.7883 - val_loss: 0.9229 - val_acc: 0.7520\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8682 - acc: 0.7885 - val_loss: 0.9294 - val_acc: 0.7540\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8716 - acc: 0.7847 - val_loss: 0.9429 - val_acc: 0.7500\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8687 - acc: 0.7899 - val_loss: 0.9246 - val_acc: 0.7600\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8687 - acc: 0.7883 - val_loss: 0.9241 - val_acc: 0.7570\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8679 - acc: 0.7903 - val_loss: 0.9293 - val_acc: 0.7540\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8678 - acc: 0.7900 - val_loss: 0.9555 - val_acc: 0.7490\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8685 - acc: 0.7883 - val_loss: 0.9349 - val_acc: 0.7500\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8673 - acc: 0.7883 - val_loss: 0.9247 - val_acc: 0.7570\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8679 - acc: 0.7880 - val_loss: 0.9379 - val_acc: 0.7520\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8666 - acc: 0.7885 - val_loss: 0.9246 - val_acc: 0.7550\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8668 - acc: 0.7883 - val_loss: 0.9256 - val_acc: 0.7560\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8658 - acc: 0.7884 - val_loss: 0.9253 - val_acc: 0.7520\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8666 - acc: 0.7892 - val_loss: 0.9260 - val_acc: 0.7570\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8669 - acc: 0.7893 - val_loss: 0.9251 - val_acc: 0.7510\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8665 - acc: 0.7876 - val_loss: 0.9206 - val_acc: 0.7610\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8667 - acc: 0.7899 - val_loss: 0.9233 - val_acc: 0.7540\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8640 - acc: 0.7879 - val_loss: 0.9292 - val_acc: 0.7460\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8642 - acc: 0.7869 - val_loss: 0.9300 - val_acc: 0.7500\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8668 - acc: 0.7885 - val_loss: 0.9209 - val_acc: 0.7570\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8642 - acc: 0.7863 - val_loss: 0.9261 - val_acc: 0.7530\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8652 - acc: 0.7891 - val_loss: 0.9318 - val_acc: 0.7510\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8639 - acc: 0.7896 - val_loss: 0.9221 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8656 - acc: 0.7884 - val_loss: 0.9268 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8654 - acc: 0.7883 - val_loss: 0.9234 - val_acc: 0.7600\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8650 - acc: 0.7879 - val_loss: 0.9202 - val_acc: 0.7580\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8642 - acc: 0.7881 - val_loss: 0.9254 - val_acc: 0.7470\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8628 - acc: 0.7908 - val_loss: 0.9322 - val_acc: 0.7460\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8650 - acc: 0.7881 - val_loss: 0.9264 - val_acc: 0.7550\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8645 - acc: 0.7888 - val_loss: 0.9411 - val_acc: 0.7530\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8645 - acc: 0.7863 - val_loss: 0.9355 - val_acc: 0.7450\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8629 - acc: 0.7899 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8643 - acc: 0.7885 - val_loss: 0.9198 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8634 - acc: 0.7875 - val_loss: 0.9452 - val_acc: 0.7400\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8628 - acc: 0.7881 - val_loss: 0.9372 - val_acc: 0.7490\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8628 - acc: 0.7876 - val_loss: 0.9238 - val_acc: 0.7570\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8624 - acc: 0.7888 - val_loss: 0.9267 - val_acc: 0.7490\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8622 - acc: 0.7900 - val_loss: 0.9221 - val_acc: 0.7590\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8610 - acc: 0.7917 - val_loss: 0.9219 - val_acc: 0.7530\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8605 - acc: 0.7903 - val_loss: 0.9543 - val_acc: 0.7380\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8628 - acc: 0.7897 - val_loss: 0.9549 - val_acc: 0.7460\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.7904 - val_loss: 0.9254 - val_acc: 0.7550\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8606 - acc: 0.7885 - val_loss: 0.9220 - val_acc: 0.7560\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8600 - acc: 0.7923 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.7911 - val_loss: 0.9358 - val_acc: 0.7550\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8610 - acc: 0.7889 - val_loss: 0.9241 - val_acc: 0.7490\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8605 - acc: 0.7887 - val_loss: 0.9251 - val_acc: 0.7520\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8583 - acc: 0.7901 - val_loss: 0.9249 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8587 - acc: 0.7911 - val_loss: 0.9202 - val_acc: 0.7570\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8586 - acc: 0.7921 - val_loss: 0.9314 - val_acc: 0.7540\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8600 - acc: 0.7863 - val_loss: 0.9215 - val_acc: 0.7530\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8588 - acc: 0.7885 - val_loss: 0.9220 - val_acc: 0.7520\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8590 - acc: 0.7916 - val_loss: 0.9211 - val_acc: 0.7590\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8604 - acc: 0.7892 - val_loss: 0.9235 - val_acc: 0.7510\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8582 - acc: 0.7925 - val_loss: 0.9174 - val_acc: 0.7600\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8592 - acc: 0.7895 - val_loss: 0.9290 - val_acc: 0.7570\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8584 - acc: 0.7909 - val_loss: 0.9189 - val_acc: 0.7540\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8576 - acc: 0.7905 - val_loss: 0.9190 - val_acc: 0.7510\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8596 - acc: 0.7909 - val_loss: 0.9206 - val_acc: 0.7590\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8603 - acc: 0.7899 - val_loss: 0.9363 - val_acc: 0.7480\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8604 - acc: 0.7891 - val_loss: 0.9222 - val_acc: 0.7590\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8600 - acc: 0.7881 - val_loss: 0.9332 - val_acc: 0.7490\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8573 - acc: 0.7919 - val_loss: 0.9240 - val_acc: 0.7570\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8591 - acc: 0.7896 - val_loss: 0.9157 - val_acc: 0.7560\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8570 - acc: 0.7917 - val_loss: 0.9149 - val_acc: 0.7570\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8564 - acc: 0.7913 - val_loss: 0.9168 - val_acc: 0.7550\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8576 - acc: 0.7897 - val_loss: 0.9162 - val_acc: 0.7610\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8558 - acc: 0.7925 - val_loss: 0.9221 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8573 - acc: 0.7900 - val_loss: 0.9235 - val_acc: 0.7590\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8594 - acc: 0.7923 - val_loss: 0.9720 - val_acc: 0.7370\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8578 - acc: 0.7905 - val_loss: 0.9212 - val_acc: 0.7540\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8574 - acc: 0.7896 - val_loss: 0.9234 - val_acc: 0.7520\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8571 - acc: 0.7908 - val_loss: 0.9351 - val_acc: 0.7520\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8556 - acc: 0.7897 - val_loss: 0.9331 - val_acc: 0.7480\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8578 - acc: 0.7921 - val_loss: 0.9127 - val_acc: 0.7580\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8544 - acc: 0.7909 - val_loss: 0.9247 - val_acc: 0.7470\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8552 - acc: 0.7920 - val_loss: 0.9188 - val_acc: 0.7570\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8535 - acc: 0.7941 - val_loss: 0.9522 - val_acc: 0.7440\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8577 - acc: 0.7892 - val_loss: 0.9134 - val_acc: 0.7560\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8556 - acc: 0.7903 - val_loss: 0.9365 - val_acc: 0.7520\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8565 - acc: 0.7904 - val_loss: 0.9634 - val_acc: 0.7410\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8574 - acc: 0.7897 - val_loss: 0.9352 - val_acc: 0.7450\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8566 - acc: 0.7952 - val_loss: 0.9396 - val_acc: 0.7560\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8563 - acc: 0.7909 - val_loss: 0.9140 - val_acc: 0.7590\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8532 - acc: 0.7929 - val_loss: 0.9146 - val_acc: 0.7570\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8557 - acc: 0.7937 - val_loss: 0.9238 - val_acc: 0.7470\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8540 - acc: 0.7932 - val_loss: 0.9222 - val_acc: 0.7520\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8561 - acc: 0.7917 - val_loss: 0.9187 - val_acc: 0.7560\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8537 - acc: 0.7927 - val_loss: 0.9235 - val_acc: 0.7510\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8538 - acc: 0.7929 - val_loss: 0.9233 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8542 - acc: 0.7916 - val_loss: 0.9229 - val_acc: 0.7630\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8538 - acc: 0.7945 - val_loss: 0.9269 - val_acc: 0.7560\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8539 - acc: 0.7943 - val_loss: 0.9459 - val_acc: 0.7480\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8530 - acc: 0.7912 - val_loss: 0.9198 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.7948 - val_loss: 0.9206 - val_acc: 0.7590\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8526 - acc: 0.7943 - val_loss: 0.9158 - val_acc: 0.7460\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8529 - acc: 0.7940 - val_loss: 0.9161 - val_acc: 0.7620\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8521 - acc: 0.7937 - val_loss: 0.9233 - val_acc: 0.7560\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8531 - acc: 0.7920 - val_loss: 0.9609 - val_acc: 0.7440\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8542 - acc: 0.7921 - val_loss: 0.9299 - val_acc: 0.7530\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8535 - acc: 0.7916 - val_loss: 0.9145 - val_acc: 0.7530\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8518 - acc: 0.7923 - val_loss: 0.9238 - val_acc: 0.7520\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8534 - acc: 0.7919 - val_loss: 0.9307 - val_acc: 0.7560\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8526 - acc: 0.7917 - val_loss: 0.9301 - val_acc: 0.7500\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8493 - acc: 0.7932 - val_loss: 0.9396 - val_acc: 0.7510\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8518 - acc: 0.7937 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8530 - acc: 0.7919 - val_loss: 0.9237 - val_acc: 0.7510\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8525 - acc: 0.7923 - val_loss: 0.9178 - val_acc: 0.7570\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8499 - acc: 0.7929 - val_loss: 0.9140 - val_acc: 0.7570\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8491 - acc: 0.7937 - val_loss: 0.9154 - val_acc: 0.7500\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8508 - acc: 0.7951 - val_loss: 0.9232 - val_acc: 0.7580\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8505 - acc: 0.7935 - val_loss: 0.9162 - val_acc: 0.7540\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8518 - acc: 0.7923 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8505 - acc: 0.7932 - val_loss: 0.9246 - val_acc: 0.7540\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.7929 - val_loss: 0.9130 - val_acc: 0.7580\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8537 - acc: 0.7913 - val_loss: 0.9471 - val_acc: 0.7520\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8518 - acc: 0.7935 - val_loss: 0.9204 - val_acc: 0.7570\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8490 - acc: 0.7945 - val_loss: 0.9139 - val_acc: 0.7490\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8492 - acc: 0.7952 - val_loss: 0.9145 - val_acc: 0.7530\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8482 - acc: 0.7939 - val_loss: 0.9265 - val_acc: 0.7610\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8502 - acc: 0.7929 - val_loss: 0.9091 - val_acc: 0.7530\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8497 - acc: 0.7944 - val_loss: 0.9239 - val_acc: 0.7620\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8501 - acc: 0.7920 - val_loss: 0.9263 - val_acc: 0.7540\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8520 - acc: 0.7917 - val_loss: 0.9302 - val_acc: 0.7500\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.7936 - val_loss: 0.9168 - val_acc: 0.7540\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8476 - acc: 0.7967 - val_loss: 0.9297 - val_acc: 0.7520\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8479 - acc: 0.7955 - val_loss: 0.9419 - val_acc: 0.7520\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8517 - acc: 0.7928 - val_loss: 0.9222 - val_acc: 0.7550\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8493 - acc: 0.7949 - val_loss: 0.9244 - val_acc: 0.7490\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8522 - acc: 0.7939 - val_loss: 0.9168 - val_acc: 0.7590\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8514 - acc: 0.7933 - val_loss: 0.9267 - val_acc: 0.7550\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8504 - acc: 0.7948 - val_loss: 0.9177 - val_acc: 0.7510\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8521 - acc: 0.7935 - val_loss: 0.9320 - val_acc: 0.7590\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8482 - acc: 0.7955 - val_loss: 0.9216 - val_acc: 0.7580\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8486 - acc: 0.7968 - val_loss: 0.9676 - val_acc: 0.7430\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8475 - acc: 0.7959 - val_loss: 0.9246 - val_acc: 0.7520\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8469 - acc: 0.7933 - val_loss: 0.9148 - val_acc: 0.7550\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8493 - acc: 0.7940 - val_loss: 0.9128 - val_acc: 0.7570\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8497 - acc: 0.7937 - val_loss: 0.9262 - val_acc: 0.7620\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8454 - acc: 0.7975 - val_loss: 0.9434 - val_acc: 0.7510\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8485 - acc: 0.7924 - val_loss: 0.9227 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8478 - acc: 0.7929 - val_loss: 0.9318 - val_acc: 0.7540\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8479 - acc: 0.7932 - val_loss: 0.9324 - val_acc: 0.7410\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8460 - acc: 0.7939 - val_loss: 0.9189 - val_acc: 0.7550\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8479 - acc: 0.7944 - val_loss: 0.9196 - val_acc: 0.7610\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8460 - acc: 0.7932 - val_loss: 0.9152 - val_acc: 0.7650\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8453 - acc: 0.7948 - val_loss: 0.9219 - val_acc: 0.7580\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8447 - acc: 0.7983 - val_loss: 0.9162 - val_acc: 0.7550\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8469 - acc: 0.7947 - val_loss: 0.9301 - val_acc: 0.7450\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8451 - acc: 0.7967 - val_loss: 0.9124 - val_acc: 0.7600\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8456 - acc: 0.7969 - val_loss: 0.9286 - val_acc: 0.7500\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8432 - acc: 0.7967 - val_loss: 0.9127 - val_acc: 0.7620\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8452 - acc: 0.7953 - val_loss: 0.9152 - val_acc: 0.7530\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8446 - acc: 0.7955 - val_loss: 0.9209 - val_acc: 0.7560\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8434 - acc: 0.7960 - val_loss: 0.9846 - val_acc: 0.7440\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8487 - acc: 0.7963 - val_loss: 0.9343 - val_acc: 0.7520\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8441 - acc: 0.7967 - val_loss: 0.9125 - val_acc: 0.7590\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8428 - acc: 0.7997 - val_loss: 0.9239 - val_acc: 0.7580\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8444 - acc: 0.7969 - val_loss: 0.9250 - val_acc: 0.7550\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8425 - acc: 0.7977 - val_loss: 0.9120 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8445 - acc: 0.7949 - val_loss: 0.9122 - val_acc: 0.7560\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8426 - acc: 0.7953 - val_loss: 0.9102 - val_acc: 0.7510\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8424 - acc: 0.7989 - val_loss: 0.9140 - val_acc: 0.7540\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.7975 - val_loss: 0.9326 - val_acc: 0.7580\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8450 - acc: 0.7963 - val_loss: 0.9278 - val_acc: 0.7530\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8449 - acc: 0.7964 - val_loss: 0.9167 - val_acc: 0.7470\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8427 - acc: 0.7963 - val_loss: 0.9144 - val_acc: 0.7530\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8479 - acc: 0.7945 - val_loss: 0.9134 - val_acc: 0.7490\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8434 - acc: 0.7965 - val_loss: 0.9168 - val_acc: 0.7570\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8425 - acc: 0.7976 - val_loss: 0.9117 - val_acc: 0.7630\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8432 - acc: 0.7976 - val_loss: 0.9306 - val_acc: 0.7580\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8434 - acc: 0.7977 - val_loss: 0.9114 - val_acc: 0.7560\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8420 - acc: 0.7948 - val_loss: 0.9218 - val_acc: 0.7590\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8421 - acc: 0.7983 - val_loss: 0.9133 - val_acc: 0.7480\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8426 - acc: 0.7968 - val_loss: 0.9128 - val_acc: 0.7600\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.7992 - val_loss: 0.9338 - val_acc: 0.7580\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8424 - acc: 0.7935 - val_loss: 0.9185 - val_acc: 0.7530\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8445 - acc: 0.7987 - val_loss: 0.9536 - val_acc: 0.7460\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8415 - acc: 0.7991 - val_loss: 0.9084 - val_acc: 0.7600\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8408 - acc: 0.7961 - val_loss: 0.9107 - val_acc: 0.7570\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8395 - acc: 0.8017 - val_loss: 0.9168 - val_acc: 0.7510\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8414 - acc: 0.7979 - val_loss: 0.9350 - val_acc: 0.7590\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8394 - acc: 0.7989 - val_loss: 0.9141 - val_acc: 0.7530\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8428 - acc: 0.7964 - val_loss: 0.9280 - val_acc: 0.7600\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8423 - acc: 0.7980 - val_loss: 0.9271 - val_acc: 0.7550\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8405 - acc: 0.7979 - val_loss: 0.9170 - val_acc: 0.7610\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8386 - acc: 0.7981 - val_loss: 0.9138 - val_acc: 0.7620\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8468 - acc: 0.7959 - val_loss: 0.9134 - val_acc: 0.7610\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8423 - acc: 0.7936 - val_loss: 0.9240 - val_acc: 0.7550\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8429 - acc: 0.7949 - val_loss: 0.9256 - val_acc: 0.7530\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8402 - acc: 0.7988 - val_loss: 0.9128 - val_acc: 0.7590\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8381 - acc: 0.8005 - val_loss: 0.9591 - val_acc: 0.7460\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8416 - acc: 0.7973 - val_loss: 0.9145 - val_acc: 0.7600\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8401 - acc: 0.7975 - val_loss: 0.9270 - val_acc: 0.7590\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8386 - acc: 0.7976 - val_loss: 0.9165 - val_acc: 0.7550\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8403 - acc: 0.8007 - val_loss: 0.9435 - val_acc: 0.7560\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8439 - acc: 0.7996 - val_loss: 0.9201 - val_acc: 0.7560\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8422 - acc: 0.7969 - val_loss: 0.9174 - val_acc: 0.7550\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8410 - acc: 0.7969 - val_loss: 0.9043 - val_acc: 0.7590\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8392 - acc: 0.7991 - val_loss: 1.0069 - val_acc: 0.7330\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.7967 - val_loss: 0.9092 - val_acc: 0.7640\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8376 - acc: 0.7975 - val_loss: 0.9244 - val_acc: 0.7540\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8398 - acc: 0.7956 - val_loss: 0.9353 - val_acc: 0.7460\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8383 - acc: 0.7989 - val_loss: 0.9055 - val_acc: 0.7610\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8404 - acc: 0.7969 - val_loss: 0.9090 - val_acc: 0.7680\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.7988 - val_loss: 0.9176 - val_acc: 0.7660\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8362 - acc: 0.8019 - val_loss: 0.9061 - val_acc: 0.7560\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8348 - acc: 0.7984 - val_loss: 0.9162 - val_acc: 0.7600\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8381 - acc: 0.8004 - val_loss: 0.9255 - val_acc: 0.7500\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8373 - acc: 0.7989 - val_loss: 0.9216 - val_acc: 0.7520\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8410 - acc: 0.8020 - val_loss: 0.9113 - val_acc: 0.7610\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8374 - acc: 0.7989 - val_loss: 0.9091 - val_acc: 0.7620\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8336 - acc: 0.8004 - val_loss: 0.9168 - val_acc: 0.7470\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8377 - acc: 0.8017 - val_loss: 0.9349 - val_acc: 0.7520\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8405 - acc: 0.8004 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8382 - acc: 0.7993 - val_loss: 0.9152 - val_acc: 0.7560\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8349 - acc: 0.8012 - val_loss: 0.9252 - val_acc: 0.7530\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8425 - acc: 0.7976 - val_loss: 0.9199 - val_acc: 0.7590\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8367 - acc: 0.7985 - val_loss: 0.9258 - val_acc: 0.7550\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8339 - acc: 0.8043 - val_loss: 0.9086 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8335 - acc: 0.8009 - val_loss: 0.9325 - val_acc: 0.7590\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8365 - acc: 0.7980 - val_loss: 0.9241 - val_acc: 0.7510\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8347 - acc: 0.8001 - val_loss: 0.9430 - val_acc: 0.7490\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.8001 - val_loss: 0.9129 - val_acc: 0.7560\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8326 - acc: 0.8027 - val_loss: 0.9290 - val_acc: 0.7500\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8381 - acc: 0.8008 - val_loss: 0.9229 - val_acc: 0.7550\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8327 - acc: 0.8015 - val_loss: 0.9144 - val_acc: 0.7600\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8376 - acc: 0.8017 - val_loss: 0.9385 - val_acc: 0.7530\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8427 - acc: 0.7964 - val_loss: 0.9168 - val_acc: 0.7600\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8357 - acc: 0.8019 - val_loss: 0.9133 - val_acc: 0.7610\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8356 - acc: 0.8020 - val_loss: 0.9283 - val_acc: 0.7590\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8344 - acc: 0.8017 - val_loss: 0.9407 - val_acc: 0.7420\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8380 - acc: 0.7972 - val_loss: 0.9188 - val_acc: 0.7530\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8349 - acc: 0.8007 - val_loss: 0.9449 - val_acc: 0.7410\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8356 - acc: 0.8021 - val_loss: 0.9348 - val_acc: 0.7480\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8348 - acc: 0.8017 - val_loss: 0.9186 - val_acc: 0.7490\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8346 - acc: 0.7995 - val_loss: 0.9133 - val_acc: 0.7660\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8360 - acc: 0.8029 - val_loss: 0.9179 - val_acc: 0.7570\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8372 - acc: 0.7960 - val_loss: 0.9193 - val_acc: 0.7520\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8324 - acc: 0.8032 - val_loss: 0.9208 - val_acc: 0.7560\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8389 - acc: 0.7981 - val_loss: 0.9341 - val_acc: 0.7540\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8356 - acc: 0.8017 - val_loss: 0.9157 - val_acc: 0.7620\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8319 - acc: 0.8072 - val_loss: 0.9326 - val_acc: 0.7600\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8345 - acc: 0.8019 - val_loss: 0.9141 - val_acc: 0.7560\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8321 - acc: 0.8020 - val_loss: 0.9153 - val_acc: 0.7550\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8317 - acc: 0.8027 - val_loss: 0.9154 - val_acc: 0.7580\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8331 - acc: 0.8019 - val_loss: 0.9147 - val_acc: 0.7620\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8344 - acc: 0.8001 - val_loss: 0.9120 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8311 - acc: 0.8039 - val_loss: 0.9030 - val_acc: 0.7660\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8360 - acc: 0.8005 - val_loss: 0.9063 - val_acc: 0.7580\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8376 - acc: 0.8011 - val_loss: 0.9100 - val_acc: 0.7530\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8337 - acc: 0.8021 - val_loss: 0.9138 - val_acc: 0.7550\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8330 - acc: 0.7999 - val_loss: 0.9343 - val_acc: 0.7530\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8322 - acc: 0.7995 - val_loss: 0.9146 - val_acc: 0.7590\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8324 - acc: 0.8031 - val_loss: 0.9125 - val_acc: 0.7560\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8349 - acc: 0.8008 - val_loss: 0.9155 - val_acc: 0.7550\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.8031 - val_loss: 0.9700 - val_acc: 0.7440\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8345 - acc: 0.7977 - val_loss: 0.9796 - val_acc: 0.7370\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8378 - acc: 0.7956 - val_loss: 0.9186 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8359 - acc: 0.8016 - val_loss: 0.9224 - val_acc: 0.7500\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8310 - acc: 0.8000 - val_loss: 0.9260 - val_acc: 0.7560\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8332 - acc: 0.8047 - val_loss: 0.9318 - val_acc: 0.7540\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8332 - acc: 0.8011 - val_loss: 0.9131 - val_acc: 0.7520\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8358 - acc: 0.8008 - val_loss: 0.9755 - val_acc: 0.7520\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8325 - acc: 0.8000 - val_loss: 0.9074 - val_acc: 0.7610\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8295 - acc: 0.8023 - val_loss: 0.9142 - val_acc: 0.7570\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8313 - acc: 0.8048 - val_loss: 0.9287 - val_acc: 0.7560\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8309 - acc: 0.8032 - val_loss: 0.9034 - val_acc: 0.7640\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8281 - acc: 0.8016 - val_loss: 0.9295 - val_acc: 0.7530\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8306 - acc: 0.8031 - val_loss: 0.9288 - val_acc: 0.7480\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.8028 - val_loss: 0.9429 - val_acc: 0.7530\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8329 - acc: 0.8028 - val_loss: 0.9591 - val_acc: 0.7380\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8347 - acc: 0.8005 - val_loss: 0.9249 - val_acc: 0.7590\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8304 - acc: 0.8019 - val_loss: 0.9097 - val_acc: 0.7620\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8304 - acc: 0.8028 - val_loss: 0.9452 - val_acc: 0.7560\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8324 - acc: 0.8013 - val_loss: 0.9213 - val_acc: 0.7550\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8278 - acc: 0.8041 - val_loss: 0.9021 - val_acc: 0.7610\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8287 - acc: 0.8019 - val_loss: 0.9507 - val_acc: 0.7410\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8377 - acc: 0.7993 - val_loss: 0.9462 - val_acc: 0.7470\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8307 - acc: 0.8021 - val_loss: 0.9154 - val_acc: 0.7580\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8279 - acc: 0.8045 - val_loss: 0.9015 - val_acc: 0.7670\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8265 - acc: 0.8047 - val_loss: 0.9270 - val_acc: 0.7620\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8330 - acc: 0.8045 - val_loss: 0.9604 - val_acc: 0.7460\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8305 - acc: 0.8007 - val_loss: 0.9355 - val_acc: 0.7590\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8262 - acc: 0.8059 - val_loss: 0.9110 - val_acc: 0.7540\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8262 - acc: 0.8059 - val_loss: 0.9453 - val_acc: 0.7540\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.8073 - val_loss: 0.9384 - val_acc: 0.7430\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8283 - acc: 0.8091 - val_loss: 0.9174 - val_acc: 0.7610\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8332 - acc: 0.7981 - val_loss: 0.9243 - val_acc: 0.7590\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8274 - acc: 0.8037 - val_loss: 0.9307 - val_acc: 0.7540\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8247 - acc: 0.8029 - val_loss: 0.9351 - val_acc: 0.7560\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8246 - acc: 0.8077 - val_loss: 0.9539 - val_acc: 0.7560\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8379 - acc: 0.8003 - val_loss: 0.9943 - val_acc: 0.7380\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8295 - acc: 0.8032 - val_loss: 0.9135 - val_acc: 0.7610\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8251 - acc: 0.8067 - val_loss: 0.9025 - val_acc: 0.7680\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8277 - acc: 0.8020 - val_loss: 0.9214 - val_acc: 0.7580\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8299 - acc: 0.8017 - val_loss: 0.9118 - val_acc: 0.7540\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8265 - acc: 0.8048 - val_loss: 0.9080 - val_acc: 0.7630\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8304 - acc: 0.8015 - val_loss: 0.9830 - val_acc: 0.7450\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8291 - acc: 0.8007 - val_loss: 0.9507 - val_acc: 0.7520\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8265 - acc: 0.8044 - val_loss: 0.9154 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8280 - acc: 0.8059 - val_loss: 0.9091 - val_acc: 0.7680\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8315 - acc: 0.8069 - val_loss: 0.9229 - val_acc: 0.7630\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8265 - acc: 0.8031 - val_loss: 0.9552 - val_acc: 0.7490\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8274 - acc: 0.8003 - val_loss: 0.9082 - val_acc: 0.7620\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8283 - acc: 0.8032 - val_loss: 0.9126 - val_acc: 0.7630\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8273 - acc: 0.8044 - val_loss: 0.9544 - val_acc: 0.7440\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8267 - acc: 0.8051 - val_loss: 0.9454 - val_acc: 0.7550\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8250 - acc: 0.8039 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8239 - acc: 0.8081 - val_loss: 0.9234 - val_acc: 0.7580\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8279 - acc: 0.8040 - val_loss: 0.9070 - val_acc: 0.7600\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8239 - acc: 0.8080 - val_loss: 0.9102 - val_acc: 0.7590\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8237 - acc: 0.8059 - val_loss: 0.9223 - val_acc: 0.7590\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8270 - acc: 0.8045 - val_loss: 0.9085 - val_acc: 0.7650\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8238 - acc: 0.8061 - val_loss: 0.9485 - val_acc: 0.7480\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8263 - acc: 0.8041 - val_loss: 0.9152 - val_acc: 0.7580\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8233 - acc: 0.8053 - val_loss: 0.9556 - val_acc: 0.7490\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8240 - acc: 0.8069 - val_loss: 0.9160 - val_acc: 0.7630\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8271 - acc: 0.8048 - val_loss: 0.9152 - val_acc: 0.7570\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8225 - acc: 0.8104 - val_loss: 0.9209 - val_acc: 0.7530\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8227 - acc: 0.8075 - val_loss: 0.9056 - val_acc: 0.7580\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8247 - acc: 0.8053 - val_loss: 0.9275 - val_acc: 0.7540\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8258 - acc: 0.8080 - val_loss: 0.9157 - val_acc: 0.7560\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8253 - acc: 0.8029 - val_loss: 0.9144 - val_acc: 0.7550\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8244 - acc: 0.8060 - val_loss: 0.9012 - val_acc: 0.7680\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8225 - acc: 0.8076 - val_loss: 0.9229 - val_acc: 0.7600\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8284 - acc: 0.8032 - val_loss: 0.9161 - val_acc: 0.7550\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8229 - acc: 0.8061 - val_loss: 0.9295 - val_acc: 0.7550\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8285 - acc: 0.8016 - val_loss: 0.9026 - val_acc: 0.7690\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8226 - acc: 0.8063 - val_loss: 0.9244 - val_acc: 0.7600\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8280 - acc: 0.8020 - val_loss: 0.9676 - val_acc: 0.7320\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8256 - acc: 0.8004 - val_loss: 0.9207 - val_acc: 0.7580\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8267 - acc: 0.8015 - val_loss: 0.9071 - val_acc: 0.7630\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8291 - acc: 0.8040 - val_loss: 0.9009 - val_acc: 0.7580\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8299 - acc: 0.8020 - val_loss: 0.9104 - val_acc: 0.7600\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8243 - acc: 0.8048 - val_loss: 0.9094 - val_acc: 0.7570\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8225 - acc: 0.8093 - val_loss: 0.9074 - val_acc: 0.7650\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.8080 - val_loss: 1.0250 - val_acc: 0.7240\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8268 - acc: 0.8061 - val_loss: 0.9212 - val_acc: 0.7510\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8279 - acc: 0.8025 - val_loss: 0.9082 - val_acc: 0.7600\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8216 - acc: 0.8060 - val_loss: 0.9510 - val_acc: 0.7530\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8289 - acc: 0.8009 - val_loss: 0.9267 - val_acc: 0.7540\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8275 - acc: 0.8020 - val_loss: 0.9482 - val_acc: 0.7520\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8264 - acc: 0.8045 - val_loss: 0.9168 - val_acc: 0.7600\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8244 - acc: 0.8061 - val_loss: 0.9275 - val_acc: 0.7520\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8212 - acc: 0.8065 - val_loss: 0.9117 - val_acc: 0.7560\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8223 - acc: 0.8095 - val_loss: 0.9447 - val_acc: 0.7530\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8252 - acc: 0.8072 - val_loss: 0.9065 - val_acc: 0.7610\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8196 - acc: 0.8088 - val_loss: 0.9171 - val_acc: 0.7630\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8251 - acc: 0.8063 - val_loss: 0.9205 - val_acc: 0.7630\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8183 - acc: 0.8091 - val_loss: 0.9012 - val_acc: 0.7640\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8238 - acc: 0.8079 - val_loss: 0.9137 - val_acc: 0.7600\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8204 - acc: 0.8084 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.8053 - val_loss: 0.9141 - val_acc: 0.7610\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8194 - acc: 0.8075 - val_loss: 0.9103 - val_acc: 0.7610\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8217 - acc: 0.8053 - val_loss: 0.9343 - val_acc: 0.7500\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8214 - acc: 0.8071 - val_loss: 0.9717 - val_acc: 0.7470\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8308 - acc: 0.8008 - val_loss: 0.9219 - val_acc: 0.7490\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8184 - acc: 0.8076 - val_loss: 0.9051 - val_acc: 0.7640\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8171 - acc: 0.8076 - val_loss: 0.9098 - val_acc: 0.7640\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8185 - acc: 0.8084 - val_loss: 0.9248 - val_acc: 0.7510\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8295 - acc: 0.8067 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8182 - acc: 0.8092 - val_loss: 0.9053 - val_acc: 0.7550\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.8076 - val_loss: 0.9180 - val_acc: 0.7600\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8204 - acc: 0.8063 - val_loss: 0.9401 - val_acc: 0.7630\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8225 - acc: 0.8036 - val_loss: 0.9057 - val_acc: 0.7600\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8239 - acc: 0.8044 - val_loss: 0.9113 - val_acc: 0.7590\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8170 - acc: 0.8060 - val_loss: 0.9040 - val_acc: 0.7650\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8235 - acc: 0.8089 - val_loss: 0.9067 - val_acc: 0.7590\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8211 - acc: 0.8079 - val_loss: 0.9135 - val_acc: 0.7650\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n",
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.823019943300883, 0.8033333333651225]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9324050038655599, 0.7453333333333333]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.9891 - acc: 0.1400 - val_loss: 1.9461 - val_acc: 0.1640\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9617 - acc: 0.1528 - val_loss: 1.9351 - val_acc: 0.1810\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9490 - acc: 0.1617 - val_loss: 1.9285 - val_acc: 0.1890\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9413 - acc: 0.1689 - val_loss: 1.9233 - val_acc: 0.1960\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9398 - acc: 0.1713 - val_loss: 1.9192 - val_acc: 0.1990\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9335 - acc: 0.1779 - val_loss: 1.9156 - val_acc: 0.2060\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9264 - acc: 0.1897 - val_loss: 1.9110 - val_acc: 0.2100\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9216 - acc: 0.1849 - val_loss: 1.9067 - val_acc: 0.2070\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9194 - acc: 0.1935 - val_loss: 1.9027 - val_acc: 0.2100\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9107 - acc: 0.2025 - val_loss: 1.8978 - val_acc: 0.2070\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9041 - acc: 0.2024 - val_loss: 1.8916 - val_acc: 0.2170\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9008 - acc: 0.2080 - val_loss: 1.8850 - val_acc: 0.2240\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9002 - acc: 0.2105 - val_loss: 1.8783 - val_acc: 0.2260\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8849 - acc: 0.2273 - val_loss: 1.8695 - val_acc: 0.2330\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8884 - acc: 0.2192 - val_loss: 1.8616 - val_acc: 0.2390\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8747 - acc: 0.2301 - val_loss: 1.8516 - val_acc: 0.2470\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8713 - acc: 0.2320 - val_loss: 1.8410 - val_acc: 0.2700\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8708 - acc: 0.2336 - val_loss: 1.8301 - val_acc: 0.2830\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8472 - acc: 0.2524 - val_loss: 1.8162 - val_acc: 0.2890\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8367 - acc: 0.2656 - val_loss: 1.7999 - val_acc: 0.3160\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8252 - acc: 0.2679 - val_loss: 1.7838 - val_acc: 0.3320\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8119 - acc: 0.2816 - val_loss: 1.7658 - val_acc: 0.3530\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8023 - acc: 0.2800 - val_loss: 1.7466 - val_acc: 0.3860\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7919 - acc: 0.2941 - val_loss: 1.7265 - val_acc: 0.4100\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7822 - acc: 0.2984 - val_loss: 1.7072 - val_acc: 0.4290\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7573 - acc: 0.3112 - val_loss: 1.6839 - val_acc: 0.4470\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7439 - acc: 0.3209 - val_loss: 1.6612 - val_acc: 0.4750\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7326 - acc: 0.3255 - val_loss: 1.6377 - val_acc: 0.4870\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7139 - acc: 0.3407 - val_loss: 1.6142 - val_acc: 0.5200\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6907 - acc: 0.3456 - val_loss: 1.5886 - val_acc: 0.5340\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6779 - acc: 0.3489 - val_loss: 1.5638 - val_acc: 0.5570\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6653 - acc: 0.3517 - val_loss: 1.5380 - val_acc: 0.5820\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6502 - acc: 0.3660 - val_loss: 1.5141 - val_acc: 0.5950\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6353 - acc: 0.3799 - val_loss: 1.4891 - val_acc: 0.5990\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6140 - acc: 0.3916 - val_loss: 1.4650 - val_acc: 0.6060\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5990 - acc: 0.3973 - val_loss: 1.4410 - val_acc: 0.6120\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5694 - acc: 0.4079 - val_loss: 1.4150 - val_acc: 0.6160\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5512 - acc: 0.4163 - val_loss: 1.3888 - val_acc: 0.6230\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5423 - acc: 0.4121 - val_loss: 1.3654 - val_acc: 0.6370\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5254 - acc: 0.4217 - val_loss: 1.3440 - val_acc: 0.6390\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4934 - acc: 0.4364 - val_loss: 1.3184 - val_acc: 0.6410\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5046 - acc: 0.4327 - val_loss: 1.3008 - val_acc: 0.6440\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4817 - acc: 0.4427 - val_loss: 1.2795 - val_acc: 0.6490\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4598 - acc: 0.4563 - val_loss: 1.2595 - val_acc: 0.6540\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4417 - acc: 0.4584 - val_loss: 1.2390 - val_acc: 0.6490\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4236 - acc: 0.4619 - val_loss: 1.2219 - val_acc: 0.6580\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4157 - acc: 0.4695 - val_loss: 1.2051 - val_acc: 0.6650\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3991 - acc: 0.4815 - val_loss: 1.1888 - val_acc: 0.6640\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3932 - acc: 0.4720 - val_loss: 1.1739 - val_acc: 0.6750\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3727 - acc: 0.4871 - val_loss: 1.1570 - val_acc: 0.6760\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3695 - acc: 0.4833 - val_loss: 1.1414 - val_acc: 0.6840\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3570 - acc: 0.4895 - val_loss: 1.1249 - val_acc: 0.6830\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3473 - acc: 0.4969 - val_loss: 1.1136 - val_acc: 0.6900\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3334 - acc: 0.4992 - val_loss: 1.0985 - val_acc: 0.6930\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3359 - acc: 0.4948 - val_loss: 1.0876 - val_acc: 0.6940\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3132 - acc: 0.5107 - val_loss: 1.0747 - val_acc: 0.7000\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3113 - acc: 0.5048 - val_loss: 1.0621 - val_acc: 0.7060\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2930 - acc: 0.5187 - val_loss: 1.0509 - val_acc: 0.7020\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2936 - acc: 0.5168 - val_loss: 1.0413 - val_acc: 0.7070\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2746 - acc: 0.5291 - val_loss: 1.0303 - val_acc: 0.7080\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2591 - acc: 0.5363 - val_loss: 1.0184 - val_acc: 0.7130\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2522 - acc: 0.5317 - val_loss: 1.0088 - val_acc: 0.7210\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2506 - acc: 0.5351 - val_loss: 1.0017 - val_acc: 0.7180\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2349 - acc: 0.5372 - val_loss: 0.9901 - val_acc: 0.7180\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2225 - acc: 0.5412 - val_loss: 0.9803 - val_acc: 0.7170\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2112 - acc: 0.5491 - val_loss: 0.9700 - val_acc: 0.7140\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2036 - acc: 0.5513 - val_loss: 0.9573 - val_acc: 0.7200\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1985 - acc: 0.5579 - val_loss: 0.9506 - val_acc: 0.7280\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1997 - acc: 0.5541 - val_loss: 0.9452 - val_acc: 0.7220\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1904 - acc: 0.5608 - val_loss: 0.9358 - val_acc: 0.7320\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1798 - acc: 0.5641 - val_loss: 0.9273 - val_acc: 0.7330\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1788 - acc: 0.5631 - val_loss: 0.9221 - val_acc: 0.7340\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1696 - acc: 0.5665 - val_loss: 0.9149 - val_acc: 0.7400\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1694 - acc: 0.5707 - val_loss: 0.9068 - val_acc: 0.7340\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1582 - acc: 0.5656 - val_loss: 0.8993 - val_acc: 0.7390\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1556 - acc: 0.5708 - val_loss: 0.8948 - val_acc: 0.7410\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1409 - acc: 0.5791 - val_loss: 0.8860 - val_acc: 0.7410\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1335 - acc: 0.5825 - val_loss: 0.8809 - val_acc: 0.7410\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1187 - acc: 0.5949 - val_loss: 0.8715 - val_acc: 0.7450\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1257 - acc: 0.5745 - val_loss: 0.8715 - val_acc: 0.7390\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0969 - acc: 0.5964 - val_loss: 0.8605 - val_acc: 0.7460\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1071 - acc: 0.5936 - val_loss: 0.8555 - val_acc: 0.7510\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1073 - acc: 0.5868 - val_loss: 0.8512 - val_acc: 0.7470\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0927 - acc: 0.6027 - val_loss: 0.8466 - val_acc: 0.7490\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1008 - acc: 0.5913 - val_loss: 0.8392 - val_acc: 0.7500\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0910 - acc: 0.5949 - val_loss: 0.8353 - val_acc: 0.7530\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0752 - acc: 0.5992 - val_loss: 0.8304 - val_acc: 0.7510\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0828 - acc: 0.5965 - val_loss: 0.8251 - val_acc: 0.7490\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0726 - acc: 0.6001 - val_loss: 0.8194 - val_acc: 0.7570\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0748 - acc: 0.6069 - val_loss: 0.8160 - val_acc: 0.7510\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0731 - acc: 0.6017 - val_loss: 0.8144 - val_acc: 0.7570\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0445 - acc: 0.6136 - val_loss: 0.8078 - val_acc: 0.7580\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0602 - acc: 0.6096 - val_loss: 0.8036 - val_acc: 0.7560\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0523 - acc: 0.6171 - val_loss: 0.7986 - val_acc: 0.7550\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0506 - acc: 0.6160 - val_loss: 0.7973 - val_acc: 0.7550\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0318 - acc: 0.6205 - val_loss: 0.7921 - val_acc: 0.7520\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0362 - acc: 0.6161 - val_loss: 0.7858 - val_acc: 0.7570\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0287 - acc: 0.6200 - val_loss: 0.7830 - val_acc: 0.7550\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0338 - acc: 0.6203 - val_loss: 0.7783 - val_acc: 0.7550\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0179 - acc: 0.6256 - val_loss: 0.7735 - val_acc: 0.7580\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0210 - acc: 0.6295 - val_loss: 0.7728 - val_acc: 0.7580\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0048 - acc: 0.6300 - val_loss: 0.7646 - val_acc: 0.7610\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0085 - acc: 0.6276 - val_loss: 0.7617 - val_acc: 0.7560\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0103 - acc: 0.6344 - val_loss: 0.7571 - val_acc: 0.7600\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9953 - acc: 0.6343 - val_loss: 0.7538 - val_acc: 0.7630\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9893 - acc: 0.6353 - val_loss: 0.7521 - val_acc: 0.7570\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9897 - acc: 0.6368 - val_loss: 0.7472 - val_acc: 0.7610\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9961 - acc: 0.6329 - val_loss: 0.7455 - val_acc: 0.7600\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9897 - acc: 0.6345 - val_loss: 0.7475 - val_acc: 0.7610\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9880 - acc: 0.6304 - val_loss: 0.7421 - val_acc: 0.7600\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9698 - acc: 0.6499 - val_loss: 0.7374 - val_acc: 0.7630\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9689 - acc: 0.6423 - val_loss: 0.7359 - val_acc: 0.7650\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9712 - acc: 0.6427 - val_loss: 0.7311 - val_acc: 0.7620\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9812 - acc: 0.6427 - val_loss: 0.7313 - val_acc: 0.7590\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9655 - acc: 0.6491 - val_loss: 0.7312 - val_acc: 0.7620\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9656 - acc: 0.6457 - val_loss: 0.7268 - val_acc: 0.7620\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9581 - acc: 0.6511 - val_loss: 0.7238 - val_acc: 0.7590\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9650 - acc: 0.6544 - val_loss: 0.7190 - val_acc: 0.7610\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9585 - acc: 0.6476 - val_loss: 0.7155 - val_acc: 0.7560\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9572 - acc: 0.6508 - val_loss: 0.7174 - val_acc: 0.7580\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9239 - acc: 0.6553 - val_loss: 0.7106 - val_acc: 0.7600\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9409 - acc: 0.6537 - val_loss: 0.7101 - val_acc: 0.7610\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9421 - acc: 0.6564 - val_loss: 0.7080 - val_acc: 0.7640\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9501 - acc: 0.6503 - val_loss: 0.7078 - val_acc: 0.7630\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9452 - acc: 0.6516 - val_loss: 0.7055 - val_acc: 0.7590\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9484 - acc: 0.6543 - val_loss: 0.7054 - val_acc: 0.7590\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9394 - acc: 0.6581 - val_loss: 0.7023 - val_acc: 0.7620\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9240 - acc: 0.6587 - val_loss: 0.6965 - val_acc: 0.7650\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9347 - acc: 0.6587 - val_loss: 0.6943 - val_acc: 0.7630\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9073 - acc: 0.6660 - val_loss: 0.6926 - val_acc: 0.7610\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9256 - acc: 0.6601 - val_loss: 0.6894 - val_acc: 0.7680\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9197 - acc: 0.6687 - val_loss: 0.6890 - val_acc: 0.7650\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9104 - acc: 0.6737 - val_loss: 0.6860 - val_acc: 0.7600\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.6627 - val_loss: 0.6842 - val_acc: 0.7580\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9108 - acc: 0.6752 - val_loss: 0.6813 - val_acc: 0.7610\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9121 - acc: 0.6677 - val_loss: 0.6840 - val_acc: 0.7590\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9092 - acc: 0.6695 - val_loss: 0.6815 - val_acc: 0.7630\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9055 - acc: 0.6661 - val_loss: 0.6813 - val_acc: 0.7630\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9018 - acc: 0.6727 - val_loss: 0.6775 - val_acc: 0.7640\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9093 - acc: 0.6679 - val_loss: 0.6756 - val_acc: 0.7640\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8909 - acc: 0.6768 - val_loss: 0.6715 - val_acc: 0.7620\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8779 - acc: 0.6685 - val_loss: 0.6700 - val_acc: 0.7630\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8892 - acc: 0.6721 - val_loss: 0.6721 - val_acc: 0.7640\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8807 - acc: 0.6804 - val_loss: 0.6681 - val_acc: 0.7610\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8955 - acc: 0.6680 - val_loss: 0.6670 - val_acc: 0.7640\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8801 - acc: 0.6743 - val_loss: 0.6656 - val_acc: 0.7660\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8890 - acc: 0.6721 - val_loss: 0.6616 - val_acc: 0.7660\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8831 - acc: 0.6724 - val_loss: 0.6620 - val_acc: 0.7670\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8760 - acc: 0.6812 - val_loss: 0.6619 - val_acc: 0.7660\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8744 - acc: 0.6812 - val_loss: 0.6588 - val_acc: 0.7620\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8679 - acc: 0.6803 - val_loss: 0.6585 - val_acc: 0.7620\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8628 - acc: 0.6827 - val_loss: 0.6561 - val_acc: 0.7650\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8588 - acc: 0.6903 - val_loss: 0.6528 - val_acc: 0.7640\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8475 - acc: 0.6892 - val_loss: 0.6497 - val_acc: 0.7650\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8535 - acc: 0.6880 - val_loss: 0.6464 - val_acc: 0.7680\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8719 - acc: 0.6789 - val_loss: 0.6486 - val_acc: 0.7660\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8532 - acc: 0.6885 - val_loss: 0.6482 - val_acc: 0.7670\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8456 - acc: 0.6865 - val_loss: 0.6482 - val_acc: 0.7660\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8544 - acc: 0.6864 - val_loss: 0.6456 - val_acc: 0.7680\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8535 - acc: 0.6868 - val_loss: 0.6453 - val_acc: 0.7670\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8452 - acc: 0.6891 - val_loss: 0.6445 - val_acc: 0.7670\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8556 - acc: 0.6871 - val_loss: 0.6430 - val_acc: 0.7680\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8464 - acc: 0.6925 - val_loss: 0.6406 - val_acc: 0.7660\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8303 - acc: 0.6912 - val_loss: 0.6393 - val_acc: 0.7610\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8419 - acc: 0.6844 - val_loss: 0.6374 - val_acc: 0.7640\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8595 - acc: 0.6899 - val_loss: 0.6383 - val_acc: 0.7710\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8125 - acc: 0.7028 - val_loss: 0.6341 - val_acc: 0.7700\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8447 - acc: 0.6885 - val_loss: 0.6339 - val_acc: 0.7710\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8264 - acc: 0.6945 - val_loss: 0.6350 - val_acc: 0.7700\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8277 - acc: 0.6992 - val_loss: 0.6344 - val_acc: 0.7730\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8351 - acc: 0.7004 - val_loss: 0.6307 - val_acc: 0.7750\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8199 - acc: 0.7033 - val_loss: 0.6337 - val_acc: 0.7720\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8182 - acc: 0.7059 - val_loss: 0.6331 - val_acc: 0.7740\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8206 - acc: 0.7013 - val_loss: 0.6300 - val_acc: 0.7740\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8106 - acc: 0.7051 - val_loss: 0.6280 - val_acc: 0.7740\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8210 - acc: 0.7035 - val_loss: 0.6287 - val_acc: 0.7730\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8201 - acc: 0.7049 - val_loss: 0.6277 - val_acc: 0.7720\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8182 - acc: 0.7076 - val_loss: 0.6262 - val_acc: 0.7760\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7991 - acc: 0.7029 - val_loss: 0.6247 - val_acc: 0.7760\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8079 - acc: 0.7047 - val_loss: 0.6244 - val_acc: 0.7740\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8240 - acc: 0.6991 - val_loss: 0.6215 - val_acc: 0.7700\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7966 - acc: 0.7080 - val_loss: 0.6209 - val_acc: 0.7740\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8038 - acc: 0.7077 - val_loss: 0.6192 - val_acc: 0.7720\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7965 - acc: 0.7049 - val_loss: 0.6195 - val_acc: 0.7770\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8140 - acc: 0.7027 - val_loss: 0.6197 - val_acc: 0.7770\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7964 - acc: 0.7061 - val_loss: 0.6177 - val_acc: 0.7760\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8037 - acc: 0.7004 - val_loss: 0.6160 - val_acc: 0.7760\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7875 - acc: 0.7135 - val_loss: 0.6155 - val_acc: 0.7730\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7978 - acc: 0.7029 - val_loss: 0.6169 - val_acc: 0.7750\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7934 - acc: 0.7125 - val_loss: 0.6170 - val_acc: 0.7720\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7864 - acc: 0.7185 - val_loss: 0.6190 - val_acc: 0.7720\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7784 - acc: 0.7132 - val_loss: 0.6128 - val_acc: 0.7740\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7900 - acc: 0.7107 - val_loss: 0.6132 - val_acc: 0.7730\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7815 - acc: 0.7152 - val_loss: 0.6127 - val_acc: 0.7780\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7897 - acc: 0.7077 - val_loss: 0.6118 - val_acc: 0.7750\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8018 - acc: 0.7105 - val_loss: 0.6119 - val_acc: 0.7790\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7803 - acc: 0.7124 - val_loss: 0.6123 - val_acc: 0.7770\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7724 - acc: 0.7168 - val_loss: 0.6091 - val_acc: 0.7780\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7881 - acc: 0.7084 - val_loss: 0.6124 - val_acc: 0.7770\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7702 - acc: 0.7151 - val_loss: 0.6098 - val_acc: 0.7820\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step\n",
      "1500/1500 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48089611903826396, 0.8373333333333334]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6319599424997966, 0.7533333338101705]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "np.random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "np.random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.9297 - acc: 0.1961 - val_loss: 1.8965 - val_acc: 0.2250\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 1.8603 - acc: 0.2513 - val_loss: 1.8172 - val_acc: 0.2753\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 1.7514 - acc: 0.3285 - val_loss: 1.6783 - val_acc: 0.3783\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.5819 - acc: 0.4491 - val_loss: 1.4963 - val_acc: 0.5113\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.3920 - acc: 0.5664 - val_loss: 1.3103 - val_acc: 0.6003\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 1.2102 - acc: 0.6373 - val_loss: 1.1411 - val_acc: 0.6453\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 1.0547 - acc: 0.6739 - val_loss: 1.0060 - val_acc: 0.6723\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.9367 - acc: 0.6970 - val_loss: 0.9086 - val_acc: 0.6913\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8532 - acc: 0.7126 - val_loss: 0.8425 - val_acc: 0.7017\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.7940 - acc: 0.7243 - val_loss: 0.7951 - val_acc: 0.7093\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.7515 - acc: 0.7338 - val_loss: 0.7628 - val_acc: 0.7183\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.7195 - acc: 0.7429 - val_loss: 0.7360 - val_acc: 0.7263\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.6944 - acc: 0.7496 - val_loss: 0.7173 - val_acc: 0.7267\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.6734 - acc: 0.7574 - val_loss: 0.7004 - val_acc: 0.7330\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.6557 - acc: 0.7639 - val_loss: 0.6871 - val_acc: 0.7350\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6401 - acc: 0.7676 - val_loss: 0.6757 - val_acc: 0.7400\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6264 - acc: 0.7723 - val_loss: 0.6650 - val_acc: 0.7440\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.6139 - acc: 0.7778 - val_loss: 0.6569 - val_acc: 0.7497\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6026 - acc: 0.7820 - val_loss: 0.6488 - val_acc: 0.7523\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5923 - acc: 0.7847 - val_loss: 0.6411 - val_acc: 0.7593\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5825 - acc: 0.7885 - val_loss: 0.6334 - val_acc: 0.7617\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5733 - acc: 0.7925 - val_loss: 0.6289 - val_acc: 0.7613\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5652 - acc: 0.7951 - val_loss: 0.6247 - val_acc: 0.7580\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5571 - acc: 0.7988 - val_loss: 0.6174 - val_acc: 0.7653\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5498 - acc: 0.8012 - val_loss: 0.6152 - val_acc: 0.7670\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5427 - acc: 0.8040 - val_loss: 0.6089 - val_acc: 0.7663\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5355 - acc: 0.8063 - val_loss: 0.6045 - val_acc: 0.7660\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5293 - acc: 0.8086 - val_loss: 0.6004 - val_acc: 0.7690\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5233 - acc: 0.8111 - val_loss: 0.6003 - val_acc: 0.7727\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5176 - acc: 0.8135 - val_loss: 0.5948 - val_acc: 0.7760\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5123 - acc: 0.8157 - val_loss: 0.5921 - val_acc: 0.7707\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5065 - acc: 0.8184 - val_loss: 0.5920 - val_acc: 0.7733\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5013 - acc: 0.8203 - val_loss: 0.5878 - val_acc: 0.7733\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4964 - acc: 0.8213 - val_loss: 0.5848 - val_acc: 0.7773\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4919 - acc: 0.8240 - val_loss: 0.5858 - val_acc: 0.7750\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4872 - acc: 0.8252 - val_loss: 0.5808 - val_acc: 0.7783\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4826 - acc: 0.8273 - val_loss: 0.5820 - val_acc: 0.7787\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4787 - acc: 0.8287 - val_loss: 0.5786 - val_acc: 0.7767\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4747 - acc: 0.8297 - val_loss: 0.5749 - val_acc: 0.7807\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4706 - acc: 0.8325 - val_loss: 0.5742 - val_acc: 0.7817\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4667 - acc: 0.8333 - val_loss: 0.5753 - val_acc: 0.7830\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4630 - acc: 0.8349 - val_loss: 0.5734 - val_acc: 0.7843\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4589 - acc: 0.8369 - val_loss: 0.5706 - val_acc: 0.7843\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4561 - acc: 0.8382 - val_loss: 0.5702 - val_acc: 0.7833\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4528 - acc: 0.8387 - val_loss: 0.5682 - val_acc: 0.7843\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4494 - acc: 0.8400 - val_loss: 0.5678 - val_acc: 0.7857\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4459 - acc: 0.8423 - val_loss: 0.5677 - val_acc: 0.7873\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4430 - acc: 0.8424 - val_loss: 0.5661 - val_acc: 0.7843\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4401 - acc: 0.8443 - val_loss: 0.5671 - val_acc: 0.7877\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4370 - acc: 0.8448 - val_loss: 0.5656 - val_acc: 0.7913\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4341 - acc: 0.8471 - val_loss: 0.5646 - val_acc: 0.7867\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4311 - acc: 0.8470 - val_loss: 0.5630 - val_acc: 0.7913\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4284 - acc: 0.8477 - val_loss: 0.5641 - val_acc: 0.7880\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4258 - acc: 0.8494 - val_loss: 0.5632 - val_acc: 0.7890\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4234 - acc: 0.8512 - val_loss: 0.5654 - val_acc: 0.7937\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4206 - acc: 0.8513 - val_loss: 0.5633 - val_acc: 0.7907\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4185 - acc: 0.8516 - val_loss: 0.5631 - val_acc: 0.7927\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4156 - acc: 0.8536 - val_loss: 0.5648 - val_acc: 0.7900\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4134 - acc: 0.8535 - val_loss: 0.5628 - val_acc: 0.7943\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4107 - acc: 0.8545 - val_loss: 0.5643 - val_acc: 0.7930\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4087 - acc: 0.8563 - val_loss: 0.5614 - val_acc: 0.7923\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4062 - acc: 0.8565 - val_loss: 0.5625 - val_acc: 0.7947\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4038 - acc: 0.8577 - val_loss: 0.5633 - val_acc: 0.7947\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4017 - acc: 0.8594 - val_loss: 0.5627 - val_acc: 0.7930\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3994 - acc: 0.8591 - val_loss: 0.5623 - val_acc: 0.7920\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3974 - acc: 0.8608 - val_loss: 0.5631 - val_acc: 0.7893\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3951 - acc: 0.8615 - val_loss: 0.5635 - val_acc: 0.7937\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3928 - acc: 0.8629 - val_loss: 0.5621 - val_acc: 0.7913\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3907 - acc: 0.8633 - val_loss: 0.5618 - val_acc: 0.7927\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3891 - acc: 0.8639 - val_loss: 0.5634 - val_acc: 0.7927\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3873 - acc: 0.8643 - val_loss: 0.5645 - val_acc: 0.7930\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3850 - acc: 0.8664 - val_loss: 0.5628 - val_acc: 0.7920\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3833 - acc: 0.8662 - val_loss: 0.5629 - val_acc: 0.7933\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3813 - acc: 0.8673 - val_loss: 0.5630 - val_acc: 0.7930\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3792 - acc: 0.8676 - val_loss: 0.5640 - val_acc: 0.7940\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3774 - acc: 0.8690 - val_loss: 0.5659 - val_acc: 0.7917\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3756 - acc: 0.8686 - val_loss: 0.5677 - val_acc: 0.7900\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3735 - acc: 0.8705 - val_loss: 0.5674 - val_acc: 0.7950\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3722 - acc: 0.8710 - val_loss: 0.5671 - val_acc: 0.7940\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3705 - acc: 0.8709 - val_loss: 0.5669 - val_acc: 0.7927\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3689 - acc: 0.8715 - val_loss: 0.5660 - val_acc: 0.7893\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3668 - acc: 0.8721 - val_loss: 0.5655 - val_acc: 0.7937\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3650 - acc: 0.8730 - val_loss: 0.5655 - val_acc: 0.7923\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3635 - acc: 0.8742 - val_loss: 0.5698 - val_acc: 0.7897\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3621 - acc: 0.8738 - val_loss: 0.5683 - val_acc: 0.7927\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3602 - acc: 0.8745 - val_loss: 0.5701 - val_acc: 0.7893\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3585 - acc: 0.8757 - val_loss: 0.5699 - val_acc: 0.7913\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3572 - acc: 0.8761 - val_loss: 0.5712 - val_acc: 0.7943\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3554 - acc: 0.8774 - val_loss: 0.5700 - val_acc: 0.7927\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3540 - acc: 0.8768 - val_loss: 0.5699 - val_acc: 0.7920\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3524 - acc: 0.8788 - val_loss: 0.5740 - val_acc: 0.7910\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3508 - acc: 0.8790 - val_loss: 0.5723 - val_acc: 0.7900\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3493 - acc: 0.8797 - val_loss: 0.5718 - val_acc: 0.7903\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3479 - acc: 0.8801 - val_loss: 0.5735 - val_acc: 0.7903\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3468 - acc: 0.8799 - val_loss: 0.5738 - val_acc: 0.7923\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3450 - acc: 0.8811 - val_loss: 0.5744 - val_acc: 0.7907\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3435 - acc: 0.8823 - val_loss: 0.5772 - val_acc: 0.7890\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3423 - acc: 0.8821 - val_loss: 0.5763 - val_acc: 0.7897\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3405 - acc: 0.8825 - val_loss: 0.5760 - val_acc: 0.7880\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3391 - acc: 0.8841 - val_loss: 0.5797 - val_acc: 0.7897\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3373 - acc: 0.8838 - val_loss: 0.5774 - val_acc: 0.7897\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3363 - acc: 0.8839 - val_loss: 0.5776 - val_acc: 0.7883\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3347 - acc: 0.8855 - val_loss: 0.5821 - val_acc: 0.7903\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3336 - acc: 0.8858 - val_loss: 0.5803 - val_acc: 0.7927\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3322 - acc: 0.8862 - val_loss: 0.5817 - val_acc: 0.7870\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3306 - acc: 0.8859 - val_loss: 0.5815 - val_acc: 0.7910\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3292 - acc: 0.8871 - val_loss: 0.5817 - val_acc: 0.7890\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.8877 - val_loss: 0.5850 - val_acc: 0.7863\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3266 - acc: 0.8876 - val_loss: 0.5849 - val_acc: 0.7880\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3254 - acc: 0.8891 - val_loss: 0.5856 - val_acc: 0.7887\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3240 - acc: 0.8888 - val_loss: 0.5873 - val_acc: 0.7870\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3227 - acc: 0.8897 - val_loss: 0.5870 - val_acc: 0.7867\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3217 - acc: 0.8897 - val_loss: 0.5867 - val_acc: 0.7883\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3199 - acc: 0.8916 - val_loss: 0.5905 - val_acc: 0.7873\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3191 - acc: 0.8912 - val_loss: 0.5905 - val_acc: 0.7870\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3174 - acc: 0.8916 - val_loss: 0.5936 - val_acc: 0.7887\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3164 - acc: 0.8918 - val_loss: 0.5947 - val_acc: 0.7897\n",
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3150 - acc: 0.8921 - val_loss: 0.5919 - val_acc: 0.7877\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3136 - acc: 0.8931 - val_loss: 0.5940 - val_acc: 0.7850\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3123 - acc: 0.8933 - val_loss: 0.5972 - val_acc: 0.7867\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 16us/step\n",
      "4000/4000 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3083401878602577, 0.8947272727272727]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5659616414308548, 0.80475]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
